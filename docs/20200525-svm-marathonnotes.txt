WHAT NEW FEATURES SHOULD WE LOOK INTO?
- Lexical Diversity (proportion of words in set that only appears once) --> Hapax. (a word that appears only once) Type-token (word that appears all the time)
- Average word length (in syllables?) --> textstat (see scores)
- NRC Emotion Lexicon
- NLTK sentiment analysis (perhaps expand contractions first)
///////////// LIWCvectorizer (to get emotion, analysis, psych, power, dom status)
- Spellchecker to count spelling mistakes
- Correlation btwn named entities & adjectivis (Giorgos idea before class) **
- Doc2Vec of each tweet to get 20/128 dimension vectors
- TwitterDNA

WHAT FEATURES SHOULD WE REMOVE FROM THE MODEL?
- Use Sklearn function to see feature importance?

WHAT CONSIDERATIONS SHOULD WE MAKE?
- TF-idf only on training dataset. See if our accuracy changes --> do we max it?
- Data augmentation: Back translation (with Spanish tweets for example? Or convert English tweets to Danish and back?). Replace words with synonyms. Shuffle tweets btwn authors. Tweet generation.
- Twitter-trained POS tagger
- Switched to stratified k-fold

MODELS TO TRY
- Gradient Boosting
- Random Forest w 500 classifiers
///// Split authors into their tweets. Feed in each tweet one by one to LSTM and final output --> classification.


Giorgos --> Doc2Vec. Looking @ use of data augmentation. KFolds with TF-IDF. Looking into cleaning up our organization & pipeline (for testing).
While generating the stuff, he notices the transformer sort of emulates features in the datasets. Kind of gives a clue as to what we should look into in the two corpuses. will send out.

Sara --> Lexical diversity. Readability scores. Mispelling. Correlations of NERs and adjectives. Look into features we want to remove. (maybe twitterDNA). Twitter-trained POS tagger.

Flora --> Emotion Lexicon & Sentiment Analysis.

Meeting Friday all day.