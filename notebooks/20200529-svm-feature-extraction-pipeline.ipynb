{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import random\n",
    "import features\n",
    "import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import go_to_project_root\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_time_augmentation(ids):\n",
    "    all_authors = data.get_raw_data()\n",
    "    _authors = [all_authors[_id] for _id in ids]\n",
    "    authors = {}\n",
    "\n",
    "    for a in _authors:\n",
    "        authors[a.author_id] = a\n",
    "\n",
    "    ones = [author for author in authors.values() if author.truth == 1]\n",
    "    zeros = [author for author in authors.values() if author.truth == 0]\n",
    "\n",
    "    tweets_1 = []\n",
    "    tweets_0 = []\n",
    "\n",
    "    for z in zeros:\n",
    "        for tweet in z.tweets:\n",
    "            tweets_0 += [tweet]\n",
    "    random.shuffle(tweets_0)\n",
    "\n",
    "    for o in ones:\n",
    "        for tweet in o.tweets:\n",
    "            tweets_1 += [tweet]\n",
    "    random.shuffle(tweets_1)\n",
    "\n",
    "    for author in zeros:\n",
    "        authors[author.author_id].tweets = []\n",
    "        for i in range(100):\n",
    "            authors[author.author_id].tweets += [tweets_0.pop(0)]\n",
    "\n",
    "    for author in ones:\n",
    "        authors[author.author_id].tweets = []\n",
    "        for i in range(100):\n",
    "            authors[author.author_id].tweets += [tweets_1.pop(0)]\n",
    "\n",
    "    for i, author in enumerate(authors.keys()):\n",
    "        authors[author].author_id = f\"shuffled-{i + 1}\"\n",
    "        \n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_augmentation(TestAuthor, n=3):\n",
    "    # Get in an author object\n",
    "    # Return a list of author objects created from a subset of tweets\n",
    "    Sub_Authors = [0] * n\n",
    "    for i in range(n):\n",
    "        # Randomly shuffle the tweets of the author\n",
    "        random.shuffle(TestAuthor.tweets)\n",
    "\n",
    "        #Save a new author object with half of the tweets\n",
    "        Sub_Authors[i] = data.Author(TestAuthor.author_id, TestAuthor.tweets[:50], TestAuthor.truth)\n",
    "        \n",
    "    _ret = {}\n",
    "    for a in Sub_Authors:\n",
    "        _ret[a.author_id] = a\n",
    "    return _ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Beginning k fold 1\nAugmenting training data.\nCleaning tweets...\nDone\nProcessing Named Entities...\nDone\nProcessing Part-Of-Speech Tags...\nDone\nThis may take some time.\n||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Features have been extracted. Now clustering.\nLoading data...\nCreating models...\n100.0% of requested models trained\nLoading data...\nCreating models...\n100.0% of requested models trained\nSaving training data.\nBeginning test time augmentation\nExtracting test author data.\nCleaning tweets...\nDone\nProcessing Named Entities...\nDone\nProcessing Part-Of-Speech Tags...\nDone\nThis may take some time.\n||\nBatch {k} finished\n\n\n"
    }
   ],
   "source": [
    "# First we take a .csv file with the author IDs and their truth values\n",
    "\n",
    "go_to_project_root()\n",
    "\n",
    "all_authors = data.get_processed_data()\n",
    "\n",
    "df = pd.read_csv(\"data/IDs_names.csv\").to_numpy()\n",
    "X = df[:,0]\n",
    "y = df[:,1].astype(int)\n",
    "\n",
    "PIPELINE_PATH = \"data/processed/\"\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3,shuffle=True,random_state=69)\n",
    "\n",
    "#Start counting so we know in which fold we are in\n",
    "k = 1\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    print(\"Beginning k fold {}\".format(k))\n",
    "    \n",
    "    ############################ TRAINING ##############################\n",
    "    \n",
    "    Train_Authors = {}\n",
    "    for a in [all_authors[_id] for _id in X[train_index]]:\n",
    "        Train_Authors[a.author_id] = a\n",
    "\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    print(\"Augmenting training data.\")\n",
    "    \n",
    "    # Augment training data. Then extract the features for it\n",
    "    augmentations = train_time_augmentation(X[train_index])\n",
    "    \n",
    "    # # First extract the nonlinguistic features\n",
    "    augmentations = features.extract_nonlinguistic_features(augmentations)\n",
    "\n",
    "    # # Extract semantic similarity\n",
    "    augmentations = features.extract_semantic_similarity(augmentations, model=nlp)\n",
    "\n",
    "    # # Get the lemmas\n",
    "    augmentations = features.extract_clean_tweets(augmentations, model=nlp)\n",
    "\n",
    "    # # Lexical features -- TTR requires lemmas\n",
    "    augmentations = features.extract_lexical_features(augmentations)\n",
    "\n",
    "    # # Get Named Entities\n",
    "    augmentations = features.extract_named_entities(augmentations, model=nlp)\n",
    "\n",
    "    # # Get POS tags\n",
    "    augmentations = features.extract_pos_tags(augmentations, model=nlp)\n",
    "\n",
    "    # # Count POSes and get adjectives\n",
    "    augmentations = features.extract_POS_features(augmentations, model=nlp)\n",
    "\n",
    "    # # Extract emotions\n",
    "    augmentations = features.extract_emotion_features(augmentations)\n",
    "    \n",
    "    ################# FEATURES ALL EXTRACTED ############\n",
    "    \n",
    "    print(\"Features have been extracted. Now clustering.\")\n",
    "    \n",
    "    Train_Authors.update(augmentations)\n",
    "    \n",
    "    # Cluster the Named Entities\n",
    "    Train_Authors, ner_clusters = features.extract_mcts_ner(Train_Authors)\n",
    "    \n",
    "    # Cluster the adjectives\n",
    "    Train_Authors, adj_clusters = features.extract_mcts_adj(Train_Authors)\n",
    "\n",
    "    # Create dataframe of what\n",
    "    train_df = preprocessing.convert_to_df(Train_Authors)\n",
    "    \n",
    "    \n",
    "    train_df = train_df.drop('author_id', axis=1).to_numpy()\n",
    "    X_train = train_df[:,:-1]\n",
    "    \n",
    "    THIS_PIPELINE_PATH = PIPELINE_PATH + \"K{}/\".format(k)\n",
    "\n",
    "    if not os.path.exists(THIS_PIPELINE_PATH):\n",
    "        os.makedirs(THIS_PIPELINE_PATH)\n",
    "        print(f\"Created directory: {THIS_PIPELINE_PATH}\")\n",
    "    \n",
    "    print(\"Saving training data.\")\n",
    "    \n",
    "    pd.DataFrame(X_train).to_csv(THIS_PIPELINE_PATH+\"X_train.csv\")\n",
    "    pd.DataFrame(y_train).to_csv(THIS_PIPELINE_PATH+\"y_train.csv\")\n",
    "    \n",
    "    # Write clusters (if you want to further modularize this. you can save the test and train indices and separate these two parts of the feature extraction\n",
    "    with open(THIS_PIPELINE_PATH + 'ner_clusters.txt', 'w', encoding='utf-8') as f:\n",
    "        for item in ner_clusters:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "        \n",
    "    with open(THIS_PIPELINE_PATH + 'adj_clusters.txt', 'w', encoding='utf-8') as f:\n",
    "        for item in adj_clusters:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "        \n",
    "    ############################ TESTING DATA ##############################\n",
    "    \n",
    "    y_test = y[test_index]\n",
    "    # preds = []\n",
    "    \n",
    "    # Save y_test values\n",
    "    pd.DataFrame(y_test).to_csv(THIS_PIPELINE_PATH+\"y_test.csv\")\n",
    "\n",
    "    print(\"Beginning test time augmentation\")\n",
    "    \n",
    "    # First get all of the test authors\n",
    "    TestAuthors = {}\n",
    "    for a in [all_authors[_id] for _id in X[test_index]]:\n",
    "        TestAuthors[a.author_id] = a\n",
    "    \n",
    "    # Now to augment the test data\n",
    "    \n",
    "    # Go through each test data point once at a time\n",
    "    print(\"Extracting test author data.\")\n",
    "    for author in X[test_index]:\n",
    "        Test3s_Authors = test_time_augmentation(TestAuthors[author])\n",
    "    \n",
    "        # We now have turned one test author into a dictionary of three authors.\n",
    "\n",
    "        # First extract the nonlinguistic features\n",
    "        Test3s_Authors = features.extract_nonlinguistic_features(Test3s_Authors)\n",
    "\n",
    "        # Extract semantic similarity\n",
    "        Test3s_Authors = features.extract_semantic_similarity(Test3s_Authors, model=nlp)\n",
    "\n",
    "        # Get the lemmas\n",
    "        Test3s_Authors = features.extract_clean_tweets(Test3s_Authors, model=nlp)\n",
    "\n",
    "        # Lexical features -- TTR requires lemmas\n",
    "        Test3s_Authors = features.extract_lexical_features(Test3s_Authors)\n",
    "\n",
    "        # Get Named Entities\n",
    "        Test3s_Authors = features.extract_named_entities(Test3s_Authors, model=nlp)\n",
    "\n",
    "        # Cluster the Named Entities\n",
    "        Test3s_Authors = features.extract_mcts_ner(Test3s_Authors, c=ner_clusters) ## NEW FUNCTION HERE\n",
    "\n",
    "        # Get POS tags\n",
    "        Test3s_Authors = features.extract_pos_tags(Test3s_Authors, model=nlp)\n",
    "\n",
    "        # Count POSes and get adjectives\n",
    "        Test3s_Authors = features.extract_POS_features(Test3s_Authors, model=nlp)\n",
    "\n",
    "        # Cluster the adjectives\n",
    "        Test3s_Authors = features.extract_mcts_adj(Test3s_Authors, c=adj_clusters) ## NEW FUNCTION HERE\n",
    "\n",
    "        # Extract emotions\n",
    "        Test3s_Authors = features.extract_emotion_features(Test3s_Authors)\n",
    "\n",
    "        test_df = preprocessing.convert_to_df(Test3s_Authors)\n",
    "        test_df = test_df.drop('author_id', axis=1).to_numpy()\n",
    "        X_test = test_df[:,:-1]\n",
    "        \n",
    "        THIS_PIPELINE_PATH = THIS_PIPELINE_PATH + \"X_test/\"\n",
    "\n",
    "        if not os.path.exists(THIS_PIPELINE_PATH):\n",
    "            os.makedirs(THIS_PIPELINE_PATH)\n",
    "            print(f\"Created directory: {THIS_PIPELINE_PATH}\")\n",
    "        \n",
    "        # Save all three datapoints corresponding to this author in a CSV file in a folder called \"X_test\"\n",
    "        pd.DataFrame(X_test).to_csv(THIS_PIPELINE_PATH+f\"{author}.csv\")\n",
    "        print(\"|\",end=\"\")\n",
    "        \n",
    "    print()\n",
    "    print(f\"Batch {k} finished\")\n",
    "    print()\n",
    "    print()\n",
    "    k += 1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python38064bit4539a5a599f24d04a1e10a4002ffd2cd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}