{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import features\n",
    "import data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Phrases, Word2Vec\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_embeddings(authors):\n",
    "    # Split sentences\n",
    "    sentences = [[\" \".join(tw) for tw in auth.nosw] for auth in list(authors.values())]\n",
    "\n",
    "    # Create bigram model\n",
    "    bigram_model = Phrases(sentences, min_count=1, threshold=1)\n",
    "    bigram_sentences = []\n",
    "    for sentence in sentences:\n",
    "        bigram_sentence = \" \".join(bigram_model[sentence])\n",
    "        bigram_sentences += [bigram_sentence.split(\" \")]\n",
    "\n",
    "    # Create trigram model\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "    trigram_sentences = []\n",
    "    for bigram_sentence in bigram_sentences:\n",
    "        trigram_sentence = \" \".join(trigram_model[bigram_sentence])\n",
    "        trigram_sentences += [trigram_sentence]\n",
    "\n",
    "    # Create trigram dictionary\n",
    "    trigram_dictionary = Dictionary([sentence.split(\" \") for sentence in trigram_sentences])\n",
    "    trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "    trigram_dictionary.compactify()\n",
    "\n",
    "    # initiate the model and perform the first epoch of training\n",
    "    sentences_split = [tr.split(\" \") for tr in trigram_sentences]\n",
    "    model = Word2Vec(sentences_split, size=300, window=4, min_count=1, sg=1, workers=7)\n",
    "    model.train(sentences_split, total_examples=model.corpus_count, epochs=10) # 10 epochs is good\n",
    "\n",
    "    # Get ordered vocabulary\n",
    "    ordered_vocab = [(term, voc.index, voc.count) for term, voc in model.wv.vocab.items()]\n",
    "    ordered_vocab = sorted(ordered_vocab, key=lambda p: (lambda x,y,z: (-z))(*p))\n",
    "    ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "    word_vectors = pd.DataFrame(model.wv.vectors[term_indices, :], index=ordered_terms)\n",
    "\n",
    "    \n",
    "\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = data.get_processed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                  0         1         2         3         4         5    \\\nnew         -0.369784  0.141009  0.055778 -0.418144 -0.024401  0.208131   \nsay          0.434162 -0.021810 -0.047034 -0.261422 -0.434942  0.450776   \ntrump       -0.303410 -0.322475  0.389426  0.103882  0.138253 -0.019693   \nget         -0.506700  0.154490  0.466314  0.307095 -0.203990 -0.127998   \nmake        -0.172733  0.263569 -0.477806  0.141618  0.667739  0.193659   \n...               ...       ...       ...       ...       ...       ...   \nlooot       -0.033626 -0.027094  0.061696 -0.113536  0.046585  0.067574   \nwaterparks  -0.032632 -0.000278  0.061473 -0.112335  0.052731  0.089424   \nsiren       -0.035896 -0.028629  0.050580 -0.109280  0.054341  0.091093   \ndecapitated -0.018493 -0.024038  0.064579 -0.094584  0.046242  0.072547   \ngiddings    -0.043738 -0.031395  0.054215 -0.108214  0.034889  0.055003   \n\n                  6         7         8         9    ...       290       291  \\\nnew         -0.420504  0.324528 -0.173325 -0.149505  ...  0.075717 -0.181482   \nsay         -0.017344  0.020008  0.352316 -0.314236  ...  0.325750  0.017092   \ntrump       -0.091739  0.061032 -0.263646 -1.180605  ... -0.142401 -0.441269   \nget         -0.027809 -0.329827  0.589091  0.290373  ... -0.202997  0.344854   \nmake         0.373511  0.681837 -0.155508  0.134450  ... -0.259661 -0.583516   \n...               ...       ...       ...       ...  ...       ...       ...   \nlooot       -0.011137 -0.069455  0.048118 -0.082641  ... -0.150200 -0.077837   \nwaterparks  -0.014229 -0.054276  0.036335 -0.105293  ... -0.133158 -0.069423   \nsiren       -0.014018 -0.051028  0.052495 -0.095744  ... -0.131747 -0.075568   \ndecapitated -0.015697 -0.039569  0.023840 -0.092815  ... -0.127796 -0.073799   \ngiddings    -0.003537 -0.041938  0.035497 -0.097011  ... -0.122586 -0.066027   \n\n                  292       293       294       295       296       297  \\\nnew         -0.358523 -0.247344  0.030666  0.335571 -0.116987  0.080384   \nsay          0.082372  0.557990  0.366484  0.282300  0.011835 -0.561864   \ntrump        0.045012  0.228855  0.121923  0.135348 -0.327279 -0.379233   \nget          0.092286  0.025407 -0.043116 -0.051287  0.329831  0.315833   \nmake        -0.796639  0.189896  0.280515  0.368113 -0.719639 -0.039583   \n...               ...       ...       ...       ...       ...       ...   \nlooot        0.010695  0.115523  0.053264  0.089651  0.022475 -0.028893   \nwaterparks   0.019447  0.133627  0.009680  0.116290  0.014332 -0.007247   \nsiren       -0.004079  0.120672  0.034092  0.105262  0.033650 -0.030997   \ndecapitated  0.017202  0.113157  0.047215  0.088760  0.009108 -0.035587   \ngiddings     0.004618  0.101904  0.064647  0.073743  0.008778 -0.011402   \n\n                  298       299  \nnew         -0.088567  0.037428  \nsay          0.221967  0.215300  \ntrump        0.326957  0.332165  \nget          0.116165  0.056847  \nmake         0.259604 -0.394774  \n...               ...       ...  \nlooot        0.021275  0.003953  \nwaterparks   0.027178  0.019263  \nsiren        0.020885  0.000142  \ndecapitated  0.034211  0.009424  \ngiddings     0.030256  0.016308  \n\n[28722 rows x 300 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>new</th>\n      <td>-0.369784</td>\n      <td>0.141009</td>\n      <td>0.055778</td>\n      <td>-0.418144</td>\n      <td>-0.024401</td>\n      <td>0.208131</td>\n      <td>-0.420504</td>\n      <td>0.324528</td>\n      <td>-0.173325</td>\n      <td>-0.149505</td>\n      <td>...</td>\n      <td>0.075717</td>\n      <td>-0.181482</td>\n      <td>-0.358523</td>\n      <td>-0.247344</td>\n      <td>0.030666</td>\n      <td>0.335571</td>\n      <td>-0.116987</td>\n      <td>0.080384</td>\n      <td>-0.088567</td>\n      <td>0.037428</td>\n    </tr>\n    <tr>\n      <th>say</th>\n      <td>0.434162</td>\n      <td>-0.021810</td>\n      <td>-0.047034</td>\n      <td>-0.261422</td>\n      <td>-0.434942</td>\n      <td>0.450776</td>\n      <td>-0.017344</td>\n      <td>0.020008</td>\n      <td>0.352316</td>\n      <td>-0.314236</td>\n      <td>...</td>\n      <td>0.325750</td>\n      <td>0.017092</td>\n      <td>0.082372</td>\n      <td>0.557990</td>\n      <td>0.366484</td>\n      <td>0.282300</td>\n      <td>0.011835</td>\n      <td>-0.561864</td>\n      <td>0.221967</td>\n      <td>0.215300</td>\n    </tr>\n    <tr>\n      <th>trump</th>\n      <td>-0.303410</td>\n      <td>-0.322475</td>\n      <td>0.389426</td>\n      <td>0.103882</td>\n      <td>0.138253</td>\n      <td>-0.019693</td>\n      <td>-0.091739</td>\n      <td>0.061032</td>\n      <td>-0.263646</td>\n      <td>-1.180605</td>\n      <td>...</td>\n      <td>-0.142401</td>\n      <td>-0.441269</td>\n      <td>0.045012</td>\n      <td>0.228855</td>\n      <td>0.121923</td>\n      <td>0.135348</td>\n      <td>-0.327279</td>\n      <td>-0.379233</td>\n      <td>0.326957</td>\n      <td>0.332165</td>\n    </tr>\n    <tr>\n      <th>get</th>\n      <td>-0.506700</td>\n      <td>0.154490</td>\n      <td>0.466314</td>\n      <td>0.307095</td>\n      <td>-0.203990</td>\n      <td>-0.127998</td>\n      <td>-0.027809</td>\n      <td>-0.329827</td>\n      <td>0.589091</td>\n      <td>0.290373</td>\n      <td>...</td>\n      <td>-0.202997</td>\n      <td>0.344854</td>\n      <td>0.092286</td>\n      <td>0.025407</td>\n      <td>-0.043116</td>\n      <td>-0.051287</td>\n      <td>0.329831</td>\n      <td>0.315833</td>\n      <td>0.116165</td>\n      <td>0.056847</td>\n    </tr>\n    <tr>\n      <th>make</th>\n      <td>-0.172733</td>\n      <td>0.263569</td>\n      <td>-0.477806</td>\n      <td>0.141618</td>\n      <td>0.667739</td>\n      <td>0.193659</td>\n      <td>0.373511</td>\n      <td>0.681837</td>\n      <td>-0.155508</td>\n      <td>0.134450</td>\n      <td>...</td>\n      <td>-0.259661</td>\n      <td>-0.583516</td>\n      <td>-0.796639</td>\n      <td>0.189896</td>\n      <td>0.280515</td>\n      <td>0.368113</td>\n      <td>-0.719639</td>\n      <td>-0.039583</td>\n      <td>0.259604</td>\n      <td>-0.394774</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>looot</th>\n      <td>-0.033626</td>\n      <td>-0.027094</td>\n      <td>0.061696</td>\n      <td>-0.113536</td>\n      <td>0.046585</td>\n      <td>0.067574</td>\n      <td>-0.011137</td>\n      <td>-0.069455</td>\n      <td>0.048118</td>\n      <td>-0.082641</td>\n      <td>...</td>\n      <td>-0.150200</td>\n      <td>-0.077837</td>\n      <td>0.010695</td>\n      <td>0.115523</td>\n      <td>0.053264</td>\n      <td>0.089651</td>\n      <td>0.022475</td>\n      <td>-0.028893</td>\n      <td>0.021275</td>\n      <td>0.003953</td>\n    </tr>\n    <tr>\n      <th>waterparks</th>\n      <td>-0.032632</td>\n      <td>-0.000278</td>\n      <td>0.061473</td>\n      <td>-0.112335</td>\n      <td>0.052731</td>\n      <td>0.089424</td>\n      <td>-0.014229</td>\n      <td>-0.054276</td>\n      <td>0.036335</td>\n      <td>-0.105293</td>\n      <td>...</td>\n      <td>-0.133158</td>\n      <td>-0.069423</td>\n      <td>0.019447</td>\n      <td>0.133627</td>\n      <td>0.009680</td>\n      <td>0.116290</td>\n      <td>0.014332</td>\n      <td>-0.007247</td>\n      <td>0.027178</td>\n      <td>0.019263</td>\n    </tr>\n    <tr>\n      <th>siren</th>\n      <td>-0.035896</td>\n      <td>-0.028629</td>\n      <td>0.050580</td>\n      <td>-0.109280</td>\n      <td>0.054341</td>\n      <td>0.091093</td>\n      <td>-0.014018</td>\n      <td>-0.051028</td>\n      <td>0.052495</td>\n      <td>-0.095744</td>\n      <td>...</td>\n      <td>-0.131747</td>\n      <td>-0.075568</td>\n      <td>-0.004079</td>\n      <td>0.120672</td>\n      <td>0.034092</td>\n      <td>0.105262</td>\n      <td>0.033650</td>\n      <td>-0.030997</td>\n      <td>0.020885</td>\n      <td>0.000142</td>\n    </tr>\n    <tr>\n      <th>decapitated</th>\n      <td>-0.018493</td>\n      <td>-0.024038</td>\n      <td>0.064579</td>\n      <td>-0.094584</td>\n      <td>0.046242</td>\n      <td>0.072547</td>\n      <td>-0.015697</td>\n      <td>-0.039569</td>\n      <td>0.023840</td>\n      <td>-0.092815</td>\n      <td>...</td>\n      <td>-0.127796</td>\n      <td>-0.073799</td>\n      <td>0.017202</td>\n      <td>0.113157</td>\n      <td>0.047215</td>\n      <td>0.088760</td>\n      <td>0.009108</td>\n      <td>-0.035587</td>\n      <td>0.034211</td>\n      <td>0.009424</td>\n    </tr>\n    <tr>\n      <th>giddings</th>\n      <td>-0.043738</td>\n      <td>-0.031395</td>\n      <td>0.054215</td>\n      <td>-0.108214</td>\n      <td>0.034889</td>\n      <td>0.055003</td>\n      <td>-0.003537</td>\n      <td>-0.041938</td>\n      <td>0.035497</td>\n      <td>-0.097011</td>\n      <td>...</td>\n      <td>-0.122586</td>\n      <td>-0.066027</td>\n      <td>0.004618</td>\n      <td>0.101904</td>\n      <td>0.064647</td>\n      <td>0.073743</td>\n      <td>0.008778</td>\n      <td>-0.011402</td>\n      <td>0.030256</td>\n      <td>0.016308</td>\n    </tr>\n  </tbody>\n</table>\n<p>28722 rows Ã— 300 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "extract_word_embeddings(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bit4539a5a599f24d04a1e10a4002ffd2cd",
   "display_name": "Python 3.8.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}