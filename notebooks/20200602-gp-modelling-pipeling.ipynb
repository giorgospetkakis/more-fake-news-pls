{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from utils import go_to_project_root\n",
    "from scipy.stats import mode\n",
    "import data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, BernoulliRBM\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "import sklearn.svm\n",
    "import keras\n",
    "from sklearn.feature_selection import RFE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(classifier, xtrain, ytrain, xtest, ytest):\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    imp = classifier.feature_importances_\n",
    "    pred = predict(classifier, xtest)\n",
    "    return pred\n",
    "\n",
    "def feature_elim(classifier, xtrain, ytrain):\n",
    "    rfe = RFE(estimator=classifier, n_features_to_select=n_features, step=10)\n",
    "    rfe.fit(xtrain, ytrain)\n",
    "    return rfe.ranking_\n",
    "\n",
    "def get_new_xtests(path): \n",
    "    path = data_root[:-1] + \"s/\" + path\n",
    "    xtest = []\n",
    "    for file in os.listdir(path + \"X_test/\"):\n",
    "        xtest += [pd.read_csv(path + \"X_test/\" + file, index_col=0).to_numpy()[:, [2, 3, 4, 5]]]\n",
    "    return xtest\n",
    "\n",
    "def read_data(_path):\n",
    "    path = data_root + _path\n",
    "    xtrain = pd.read_csv(path + \"X_train.csv\", index_col=0).to_numpy()[:200].astype(float)\n",
    "    ytrain = pd.read_csv(path + \"y_train.csv\", index_col=0).to_numpy()[:200]\n",
    "    ytest = pd.read_csv(path + \"y_test.csv\", index_col=0).to_numpy()\n",
    "    xtest = []\n",
    "    for file in os.listdir(path + \"X_test/\"):\n",
    "        xtest += [pd.read_csv(path + \"X_test/\" + file, index_col=0).to_numpy()]\n",
    "\n",
    "    new_xt = get_new_xtests(_path)\n",
    "    mean_xt = [np.mean(xt[0], axis=0) for xt in new_xt]\n",
    "\n",
    "    for i, x in enumerate(xtest):\n",
    "        xtest[i][:,[2, 3, 4, 5]] = mean_xt[i]\n",
    "\n",
    "    return xtrain, ytrain, xtest, ytest\n",
    "\n",
    "def predict(classifier, xtest):\n",
    "    majority_vote_preds = []\n",
    "    for x in xtest:\n",
    "        x = x\n",
    "        majority_vote_preds += [np.sum(classifier.predict(x).astype(int)) > 1]\n",
    "    return majority_vote_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_to_project_root()\n",
    "data_root = \"data/processed/800/\"\n",
    "datasets = [read_data(f\"K{k+1}/\") for k in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {\n",
    "    \"lexical\": [0, 1],\n",
    "    \"semantic\": [2, 3, 4, 5],\n",
    "    \"clusters\": [6, 7],\n",
    "    \"nonling\":  list(range(8, 30)),\n",
    "    \"pos\": [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],\n",
    "    \"emotion\": [47, 48, 49, 50, 51, 52, 53, 54, 55, 56],\n",
    "    \"embeddings\": list(range(57, 357))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.7066666666666667\n"
    }
   ],
   "source": [
    "# ada = []\n",
    "# fs = feature_sets[\"embeddings\"] + feature_sets[\"emotion\"] + feature_sets[\"pos\"]\n",
    "_del = feature_sets[\"semantic\"]\n",
    "f = _del\n",
    "\n",
    "mean = 0\n",
    "for i in range(3):\n",
    "    xtrain, ytrain, xtest, ytest = datasets[i]\n",
    "    c = GradientBoostingClassifier()\n",
    "    # pred = classify(c, np.delete(xtrain, _del, axis=1), ytrain, xtest, ytest)\n",
    "    pred = classify(c, xtrain, ytrain, xtest, ytest)\n",
    "    acc = balanced_accuracy_score(ytest, pred)\n",
    "    mean += acc\n",
    "print(mean / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch = [x for x, p in enumerate([int(x == True) for x in pred]) if p != ytest[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0,\n 1,\n 12,\n 13,\n 14,\n 16,\n 22,\n 25,\n 26,\n 30,\n 33,\n 35,\n 36,\n 40,\n 41,\n 42,\n 43,\n 53,\n 58,\n 64,\n 66,\n 69,\n 70,\n 72,\n 80,\n 90,\n 94,\n 95,\n 98]"
     },
     "metadata": {},
     "execution_count": 499
    }
   ],
   "source": [
    "mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      0     1         2         3         4         5     6     7    8    \\\n0    7.50  0.98  0.963683  0.592386  0.797978  0.013333  0.00  0.42    1   \n1    8.56  1.00  0.963683  0.592386  0.797978  0.013333  0.02  0.44    2   \n2    8.82  1.00  0.963683  0.592386  0.797978  0.013333  0.02  0.38    2   \n3   11.14  1.00  0.963683  0.592386  0.797978  0.013333  0.12  0.32    2   \n4    9.14  0.92  0.963683  0.592386  0.797978  0.013333  0.00  0.52    1   \n5    9.68  1.00  0.963683  0.592386  0.797978  0.013333  0.08  0.52    1   \n6    6.68  1.00  0.963683  0.592386  0.797978  0.013333  0.04  0.38    1   \n7    9.66  1.00  0.963683  0.592386  0.797978  0.013333  0.08  0.58    1   \n8    8.64  1.00  0.963683  0.592386  0.797978  0.013333  0.18  0.48    2   \n9    6.18  0.98  0.963683  0.592386  0.797978  0.013333  0.08  0.34    2   \n10   9.44  1.00  0.963683  0.592386  0.797978  0.013333  0.06  0.28    1   \n11   7.14  1.00  0.963683  0.592386  0.797978  0.013333  0.00  0.32    2   \n12   8.38  0.98  0.963683  0.592386  0.797978  0.013333  0.08  0.28    2   \n13   7.96  1.00  0.963683  0.592386  0.797978  0.013333  0.08  0.70    2   \n14   7.06  1.00  0.963683  0.592386  0.797978  0.013333  0.00  0.72    2   \n15   6.90  1.00  0.963683  0.592386  0.797978  0.013333  0.18  0.30    2   \n16   7.40  0.94  0.963683  0.592386  0.797978  0.013333  0.08  0.20    2   \n17   7.84  1.00  0.963683  0.592386  0.797978  0.013333  0.24  0.20    2   \n18   8.62  0.96  0.963683  0.592386  0.797978  0.013333  0.06  0.30    1   \n19   6.68  1.00  0.963683  0.592386  0.797978  0.013333  0.02  0.34    2   \n20   5.72  0.98  0.963683  0.592386  0.797978  0.013333  0.02  0.46    2   \n21   6.20  0.90  0.963683  0.592386  0.797978  0.013333  0.02  0.26    1   \n22   8.68  0.98  0.963683  0.592386  0.797978  0.013333  0.08  0.24    1   \n23  10.00  1.00  0.963683  0.592386  0.797978  0.013333  0.00  0.72    3   \n24   6.54  0.90  0.963683  0.592386  0.797978  0.013333  0.04  0.14    2   \n25   8.34  1.00  0.963683  0.592386  0.797978  0.013333  0.06  0.30    2   \n26   7.72  1.00  0.963683  0.592386  0.797978  0.013333  0.00  0.28    2   \n27   6.70  1.00  0.963683  0.592386  0.797978  0.013333  0.00  0.14    1   \n28  10.34  1.00  0.963683  0.592386  0.797978  0.013333  0.02  0.42    2   \n29   8.06  1.00  0.963683  0.592386  0.797978  0.013333  0.02  0.36    2   \n30   5.28  1.00  0.963683  0.592386  0.797978  0.013333  0.00  0.18    2   \n\n     9    ...       347       348       349       350       351       352  \\\n0   1.00  ...  0.284624 -0.013412 -0.195229  0.184144  0.188326 -0.020228   \n1   1.56  ...  0.260419 -0.010028 -0.174950  0.140882  0.109642  0.052822   \n2   1.96  ...  0.198711 -0.000020 -0.145692  0.158541  0.117234  0.100025   \n3   1.06  ...  0.209349  0.012015 -0.176687  0.286394  0.081167  0.059793   \n4   1.00  ...  0.281274 -0.022952 -0.132948  0.210196  0.102468  0.023697   \n5   1.00  ...  0.288692 -0.031779 -0.160704  0.212836  0.178110  0.068823   \n6   1.00  ...  0.282537  0.003542 -0.088421  0.117531  0.100682  0.071690   \n7   1.00  ...  0.283849 -0.040608 -0.153684  0.196859  0.214526  0.037496   \n8   1.02  ...  0.227369  0.002175 -0.141336  0.102787  0.127321  0.059000   \n9   0.92  ...  0.283571  0.020827 -0.149753  0.219545  0.085846  0.129766   \n10  1.00  ...  0.199048 -0.020359 -0.111508  0.193598  0.116383  0.080548   \n11  2.00  ...  0.268049  0.010376 -0.059886  0.197630  0.046856  0.046871   \n12  1.02  ...  0.216314  0.003231 -0.107506  0.173707  0.108290  0.073758   \n13  1.68  ...  0.212923  0.051712 -0.108884  0.161840  0.117847  0.047866   \n14  1.84  ...  0.254418 -0.065668 -0.144483  0.204487  0.166599  0.025474   \n15  0.96  ...  0.229845  0.006435 -0.093976  0.144969  0.111061  0.084076   \n16  1.80  ...  0.222071 -0.012069 -0.102431  0.134223  0.151856  0.058861   \n17  1.24  ...  0.232183  0.044526 -0.243043  0.236431  0.096910  0.061974   \n18  1.00  ...  0.288066  0.004932 -0.128224  0.173980  0.149861 -0.022319   \n19  0.70  ...  0.211974 -0.034502 -0.107806  0.134025  0.031485  0.132725   \n20  1.92  ...  0.247103 -0.020387 -0.091617  0.151275  0.163580  0.004306   \n21  1.00  ...  0.274672  0.010029 -0.163421  0.142062  0.155316  0.030203   \n22  1.00  ...  0.324256 -0.038353 -0.176213  0.181269  0.150709 -0.000456   \n23  2.00  ...  0.169335 -0.016316 -0.164363  0.180981  0.121270  0.064859   \n24  1.98  ...  0.284313 -0.055719 -0.115375  0.142465  0.085027  0.009986   \n25  0.38  ...  0.248382  0.000612 -0.135905  0.210191  0.069763  0.089686   \n26  1.30  ...  0.286252  0.017708 -0.153523  0.180665  0.119512  0.023961   \n27  0.94  ...  0.295409 -0.034407 -0.096428  0.168383  0.064764 -0.008872   \n28  1.04  ...  0.251318 -0.028887 -0.100963  0.110775  0.062870  0.080216   \n29  1.24  ...  0.208101 -0.012976 -0.101445  0.178550  0.122885  0.045357   \n30  1.02  ...  0.265922  0.005143 -0.095884  0.162614  0.190332  0.003982   \n\n         353       354       355       356  \n0   0.040004 -0.008719 -0.048525  0.054909  \n1   0.011637 -0.034743 -0.029555  0.092662  \n2  -0.045165  0.055751 -0.059186  0.031795  \n3  -0.025541  0.060909 -0.089637  0.010788  \n4   0.003421 -0.011731 -0.110694  0.041663  \n5   0.014393 -0.012220 -0.123156  0.017638  \n6  -0.022632  0.019563 -0.076970  0.059577  \n7   0.033136 -0.019455 -0.075675  0.037446  \n8  -0.037056  0.006883 -0.060917  0.081950  \n9  -0.022316  0.037707 -0.102794 -0.003213  \n10 -0.029447  0.064245 -0.042525  0.020361  \n11  0.009483 -0.026331 -0.024754  0.073309  \n12 -0.039887  0.040109 -0.050138  0.055940  \n13 -0.034609  0.068680 -0.021110  0.088822  \n14 -0.009587 -0.055404 -0.097356  0.100250  \n15 -0.042976  0.055162 -0.051970  0.048090  \n16 -0.052971  0.067680 -0.039791  0.041178  \n17 -0.010752  0.055054 -0.079126 -0.005435  \n18  0.032107  0.026767 -0.052591  0.071016  \n19 -0.046978  0.006063 -0.079127  0.046315  \n20 -0.026290  0.026448 -0.089248  0.044186  \n21 -0.020394  0.018147 -0.064043  0.068041  \n22  0.068578 -0.014583 -0.097392 -0.022784  \n23 -0.056053  0.030129 -0.025244  0.083364  \n24  0.018053 -0.023819 -0.086216  0.092329  \n25 -0.029078  0.091382 -0.074203 -0.016367  \n26 -0.043953  0.051033 -0.080711  0.042857  \n27  0.005062 -0.013285 -0.070194  0.086155  \n28 -0.049728  0.017559 -0.054494  0.078599  \n29 -0.034603  0.056281 -0.088895  0.027867  \n30 -0.011848  0.016757 -0.076880  0.104585  \n\n[31 rows x 357 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>347</th>\n      <th>348</th>\n      <th>349</th>\n      <th>350</th>\n      <th>351</th>\n      <th>352</th>\n      <th>353</th>\n      <th>354</th>\n      <th>355</th>\n      <th>356</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.50</td>\n      <td>0.98</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.00</td>\n      <td>0.42</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.284624</td>\n      <td>-0.013412</td>\n      <td>-0.195229</td>\n      <td>0.184144</td>\n      <td>0.188326</td>\n      <td>-0.020228</td>\n      <td>0.040004</td>\n      <td>-0.008719</td>\n      <td>-0.048525</td>\n      <td>0.054909</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.56</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.02</td>\n      <td>0.44</td>\n      <td>2</td>\n      <td>1.56</td>\n      <td>...</td>\n      <td>0.260419</td>\n      <td>-0.010028</td>\n      <td>-0.174950</td>\n      <td>0.140882</td>\n      <td>0.109642</td>\n      <td>0.052822</td>\n      <td>0.011637</td>\n      <td>-0.034743</td>\n      <td>-0.029555</td>\n      <td>0.092662</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.82</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.02</td>\n      <td>0.38</td>\n      <td>2</td>\n      <td>1.96</td>\n      <td>...</td>\n      <td>0.198711</td>\n      <td>-0.000020</td>\n      <td>-0.145692</td>\n      <td>0.158541</td>\n      <td>0.117234</td>\n      <td>0.100025</td>\n      <td>-0.045165</td>\n      <td>0.055751</td>\n      <td>-0.059186</td>\n      <td>0.031795</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.14</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.12</td>\n      <td>0.32</td>\n      <td>2</td>\n      <td>1.06</td>\n      <td>...</td>\n      <td>0.209349</td>\n      <td>0.012015</td>\n      <td>-0.176687</td>\n      <td>0.286394</td>\n      <td>0.081167</td>\n      <td>0.059793</td>\n      <td>-0.025541</td>\n      <td>0.060909</td>\n      <td>-0.089637</td>\n      <td>0.010788</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9.14</td>\n      <td>0.92</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.00</td>\n      <td>0.52</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.281274</td>\n      <td>-0.022952</td>\n      <td>-0.132948</td>\n      <td>0.210196</td>\n      <td>0.102468</td>\n      <td>0.023697</td>\n      <td>0.003421</td>\n      <td>-0.011731</td>\n      <td>-0.110694</td>\n      <td>0.041663</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>9.68</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.08</td>\n      <td>0.52</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.288692</td>\n      <td>-0.031779</td>\n      <td>-0.160704</td>\n      <td>0.212836</td>\n      <td>0.178110</td>\n      <td>0.068823</td>\n      <td>0.014393</td>\n      <td>-0.012220</td>\n      <td>-0.123156</td>\n      <td>0.017638</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6.68</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.04</td>\n      <td>0.38</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.282537</td>\n      <td>0.003542</td>\n      <td>-0.088421</td>\n      <td>0.117531</td>\n      <td>0.100682</td>\n      <td>0.071690</td>\n      <td>-0.022632</td>\n      <td>0.019563</td>\n      <td>-0.076970</td>\n      <td>0.059577</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>9.66</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.08</td>\n      <td>0.58</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.283849</td>\n      <td>-0.040608</td>\n      <td>-0.153684</td>\n      <td>0.196859</td>\n      <td>0.214526</td>\n      <td>0.037496</td>\n      <td>0.033136</td>\n      <td>-0.019455</td>\n      <td>-0.075675</td>\n      <td>0.037446</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8.64</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.18</td>\n      <td>0.48</td>\n      <td>2</td>\n      <td>1.02</td>\n      <td>...</td>\n      <td>0.227369</td>\n      <td>0.002175</td>\n      <td>-0.141336</td>\n      <td>0.102787</td>\n      <td>0.127321</td>\n      <td>0.059000</td>\n      <td>-0.037056</td>\n      <td>0.006883</td>\n      <td>-0.060917</td>\n      <td>0.081950</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>6.18</td>\n      <td>0.98</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.08</td>\n      <td>0.34</td>\n      <td>2</td>\n      <td>0.92</td>\n      <td>...</td>\n      <td>0.283571</td>\n      <td>0.020827</td>\n      <td>-0.149753</td>\n      <td>0.219545</td>\n      <td>0.085846</td>\n      <td>0.129766</td>\n      <td>-0.022316</td>\n      <td>0.037707</td>\n      <td>-0.102794</td>\n      <td>-0.003213</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>9.44</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.06</td>\n      <td>0.28</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.199048</td>\n      <td>-0.020359</td>\n      <td>-0.111508</td>\n      <td>0.193598</td>\n      <td>0.116383</td>\n      <td>0.080548</td>\n      <td>-0.029447</td>\n      <td>0.064245</td>\n      <td>-0.042525</td>\n      <td>0.020361</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>7.14</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.00</td>\n      <td>0.32</td>\n      <td>2</td>\n      <td>2.00</td>\n      <td>...</td>\n      <td>0.268049</td>\n      <td>0.010376</td>\n      <td>-0.059886</td>\n      <td>0.197630</td>\n      <td>0.046856</td>\n      <td>0.046871</td>\n      <td>0.009483</td>\n      <td>-0.026331</td>\n      <td>-0.024754</td>\n      <td>0.073309</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>8.38</td>\n      <td>0.98</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.08</td>\n      <td>0.28</td>\n      <td>2</td>\n      <td>1.02</td>\n      <td>...</td>\n      <td>0.216314</td>\n      <td>0.003231</td>\n      <td>-0.107506</td>\n      <td>0.173707</td>\n      <td>0.108290</td>\n      <td>0.073758</td>\n      <td>-0.039887</td>\n      <td>0.040109</td>\n      <td>-0.050138</td>\n      <td>0.055940</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>7.96</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.08</td>\n      <td>0.70</td>\n      <td>2</td>\n      <td>1.68</td>\n      <td>...</td>\n      <td>0.212923</td>\n      <td>0.051712</td>\n      <td>-0.108884</td>\n      <td>0.161840</td>\n      <td>0.117847</td>\n      <td>0.047866</td>\n      <td>-0.034609</td>\n      <td>0.068680</td>\n      <td>-0.021110</td>\n      <td>0.088822</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>7.06</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.00</td>\n      <td>0.72</td>\n      <td>2</td>\n      <td>1.84</td>\n      <td>...</td>\n      <td>0.254418</td>\n      <td>-0.065668</td>\n      <td>-0.144483</td>\n      <td>0.204487</td>\n      <td>0.166599</td>\n      <td>0.025474</td>\n      <td>-0.009587</td>\n      <td>-0.055404</td>\n      <td>-0.097356</td>\n      <td>0.100250</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>6.90</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.18</td>\n      <td>0.30</td>\n      <td>2</td>\n      <td>0.96</td>\n      <td>...</td>\n      <td>0.229845</td>\n      <td>0.006435</td>\n      <td>-0.093976</td>\n      <td>0.144969</td>\n      <td>0.111061</td>\n      <td>0.084076</td>\n      <td>-0.042976</td>\n      <td>0.055162</td>\n      <td>-0.051970</td>\n      <td>0.048090</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>7.40</td>\n      <td>0.94</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.08</td>\n      <td>0.20</td>\n      <td>2</td>\n      <td>1.80</td>\n      <td>...</td>\n      <td>0.222071</td>\n      <td>-0.012069</td>\n      <td>-0.102431</td>\n      <td>0.134223</td>\n      <td>0.151856</td>\n      <td>0.058861</td>\n      <td>-0.052971</td>\n      <td>0.067680</td>\n      <td>-0.039791</td>\n      <td>0.041178</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>7.84</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.24</td>\n      <td>0.20</td>\n      <td>2</td>\n      <td>1.24</td>\n      <td>...</td>\n      <td>0.232183</td>\n      <td>0.044526</td>\n      <td>-0.243043</td>\n      <td>0.236431</td>\n      <td>0.096910</td>\n      <td>0.061974</td>\n      <td>-0.010752</td>\n      <td>0.055054</td>\n      <td>-0.079126</td>\n      <td>-0.005435</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>8.62</td>\n      <td>0.96</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.06</td>\n      <td>0.30</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.288066</td>\n      <td>0.004932</td>\n      <td>-0.128224</td>\n      <td>0.173980</td>\n      <td>0.149861</td>\n      <td>-0.022319</td>\n      <td>0.032107</td>\n      <td>0.026767</td>\n      <td>-0.052591</td>\n      <td>0.071016</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>6.68</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.02</td>\n      <td>0.34</td>\n      <td>2</td>\n      <td>0.70</td>\n      <td>...</td>\n      <td>0.211974</td>\n      <td>-0.034502</td>\n      <td>-0.107806</td>\n      <td>0.134025</td>\n      <td>0.031485</td>\n      <td>0.132725</td>\n      <td>-0.046978</td>\n      <td>0.006063</td>\n      <td>-0.079127</td>\n      <td>0.046315</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>5.72</td>\n      <td>0.98</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.02</td>\n      <td>0.46</td>\n      <td>2</td>\n      <td>1.92</td>\n      <td>...</td>\n      <td>0.247103</td>\n      <td>-0.020387</td>\n      <td>-0.091617</td>\n      <td>0.151275</td>\n      <td>0.163580</td>\n      <td>0.004306</td>\n      <td>-0.026290</td>\n      <td>0.026448</td>\n      <td>-0.089248</td>\n      <td>0.044186</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>6.20</td>\n      <td>0.90</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.02</td>\n      <td>0.26</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.274672</td>\n      <td>0.010029</td>\n      <td>-0.163421</td>\n      <td>0.142062</td>\n      <td>0.155316</td>\n      <td>0.030203</td>\n      <td>-0.020394</td>\n      <td>0.018147</td>\n      <td>-0.064043</td>\n      <td>0.068041</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>8.68</td>\n      <td>0.98</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.08</td>\n      <td>0.24</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.324256</td>\n      <td>-0.038353</td>\n      <td>-0.176213</td>\n      <td>0.181269</td>\n      <td>0.150709</td>\n      <td>-0.000456</td>\n      <td>0.068578</td>\n      <td>-0.014583</td>\n      <td>-0.097392</td>\n      <td>-0.022784</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>10.00</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.00</td>\n      <td>0.72</td>\n      <td>3</td>\n      <td>2.00</td>\n      <td>...</td>\n      <td>0.169335</td>\n      <td>-0.016316</td>\n      <td>-0.164363</td>\n      <td>0.180981</td>\n      <td>0.121270</td>\n      <td>0.064859</td>\n      <td>-0.056053</td>\n      <td>0.030129</td>\n      <td>-0.025244</td>\n      <td>0.083364</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>6.54</td>\n      <td>0.90</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.04</td>\n      <td>0.14</td>\n      <td>2</td>\n      <td>1.98</td>\n      <td>...</td>\n      <td>0.284313</td>\n      <td>-0.055719</td>\n      <td>-0.115375</td>\n      <td>0.142465</td>\n      <td>0.085027</td>\n      <td>0.009986</td>\n      <td>0.018053</td>\n      <td>-0.023819</td>\n      <td>-0.086216</td>\n      <td>0.092329</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>8.34</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.06</td>\n      <td>0.30</td>\n      <td>2</td>\n      <td>0.38</td>\n      <td>...</td>\n      <td>0.248382</td>\n      <td>0.000612</td>\n      <td>-0.135905</td>\n      <td>0.210191</td>\n      <td>0.069763</td>\n      <td>0.089686</td>\n      <td>-0.029078</td>\n      <td>0.091382</td>\n      <td>-0.074203</td>\n      <td>-0.016367</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>7.72</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.00</td>\n      <td>0.28</td>\n      <td>2</td>\n      <td>1.30</td>\n      <td>...</td>\n      <td>0.286252</td>\n      <td>0.017708</td>\n      <td>-0.153523</td>\n      <td>0.180665</td>\n      <td>0.119512</td>\n      <td>0.023961</td>\n      <td>-0.043953</td>\n      <td>0.051033</td>\n      <td>-0.080711</td>\n      <td>0.042857</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>6.70</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.00</td>\n      <td>0.14</td>\n      <td>1</td>\n      <td>0.94</td>\n      <td>...</td>\n      <td>0.295409</td>\n      <td>-0.034407</td>\n      <td>-0.096428</td>\n      <td>0.168383</td>\n      <td>0.064764</td>\n      <td>-0.008872</td>\n      <td>0.005062</td>\n      <td>-0.013285</td>\n      <td>-0.070194</td>\n      <td>0.086155</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>10.34</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.02</td>\n      <td>0.42</td>\n      <td>2</td>\n      <td>1.04</td>\n      <td>...</td>\n      <td>0.251318</td>\n      <td>-0.028887</td>\n      <td>-0.100963</td>\n      <td>0.110775</td>\n      <td>0.062870</td>\n      <td>0.080216</td>\n      <td>-0.049728</td>\n      <td>0.017559</td>\n      <td>-0.054494</td>\n      <td>0.078599</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>8.06</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.02</td>\n      <td>0.36</td>\n      <td>2</td>\n      <td>1.24</td>\n      <td>...</td>\n      <td>0.208101</td>\n      <td>-0.012976</td>\n      <td>-0.101445</td>\n      <td>0.178550</td>\n      <td>0.122885</td>\n      <td>0.045357</td>\n      <td>-0.034603</td>\n      <td>0.056281</td>\n      <td>-0.088895</td>\n      <td>0.027867</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>5.28</td>\n      <td>1.00</td>\n      <td>0.963683</td>\n      <td>0.592386</td>\n      <td>0.797978</td>\n      <td>0.013333</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>2</td>\n      <td>1.02</td>\n      <td>...</td>\n      <td>0.265922</td>\n      <td>0.005143</td>\n      <td>-0.095884</td>\n      <td>0.162614</td>\n      <td>0.190332</td>\n      <td>0.003982</td>\n      <td>-0.011848</td>\n      <td>0.016757</td>\n      <td>-0.076880</td>\n      <td>0.104585</td>\n    </tr>\n  </tbody>\n</table>\n<p>31 rows × 357 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 487
    }
   ],
   "source": [
    "pd.DataFrame([xtest[m][0] for m in mismatch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'extra' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-394-b681375bbc66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Extra Trees\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Logistic Regression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandomf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Random Forest\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# plt.plot(range(5, 300, 5), xgboost, label=\"XGBoost\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extra' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(range(5, 300, 5), extra, label=\"Extra Trees\")\n",
    "plt.plot(range(5, 300, 5), logs, label=\"Logistic Regression\")\n",
    "plt.plot(range(5, 300, 5), randomf, label=\"Random Forest\")\n",
    "# plt.plot(range(5, 300, 5), xgboost, label=\"XGBoost\")\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel(\"Number of Selected Features\", fontsize=20)\n",
    "plt.ylabel(\"Testing accuracy\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2.0300000000000002"
     },
     "metadata": {},
     "execution_count": 349
    }
   ],
   "source": [
    "0.68 + 0.73 + 0.6200000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.72\n0.7\n0.67\n"
    }
   ],
   "source": [
    "f = _del\n",
    "for i in range(3):\n",
    "    xtrain, ytrain, xtest, ytest = datasets[i]\n",
    "    mean, _  = classify(c, np.delete(xtrain, f, axis=1), ytrain, xtest, ytest)\n",
    "    print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[1],\n [2, 3, 4, 5],\n [6, 7],\n [8,\n  9,\n  10,\n  11,\n  12,\n  13,\n  14,\n  15,\n  16,\n  17,\n  18,\n  19,\n  20,\n  21,\n  22,\n  23,\n  24,\n  25,\n  26,\n  27,\n  28,\n  29],\n [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],\n [47, 48, 49, 50, 51, 52, 53, 54, 55, 56],\n [57,\n  58,\n  59,\n  60,\n  61,\n  62,\n  63,\n  64,\n  65,\n  66,\n  67,\n  68,\n  69,\n  70,\n  71,\n  72,\n  73,\n  74,\n  75,\n  76,\n  77,\n  78,\n  79,\n  80,\n  81,\n  82,\n  83,\n  84,\n  85,\n  86,\n  87,\n  88,\n  89,\n  90,\n  91,\n  92,\n  93,\n  94,\n  95,\n  96,\n  97,\n  98,\n  99,\n  100,\n  101,\n  102,\n  103,\n  104,\n  105,\n  106,\n  107,\n  108,\n  109,\n  110,\n  111,\n  112,\n  113,\n  114,\n  115,\n  116,\n  117,\n  118,\n  119,\n  120,\n  121,\n  122,\n  123,\n  124,\n  125,\n  126,\n  127,\n  128,\n  129,\n  130,\n  131,\n  132,\n  133,\n  134,\n  135,\n  136,\n  137,\n  138,\n  139,\n  140,\n  141,\n  142,\n  143,\n  144,\n  145,\n  146,\n  147,\n  148,\n  149,\n  150,\n  151,\n  152,\n  153,\n  154,\n  155,\n  156,\n  157,\n  158,\n  159,\n  160,\n  161,\n  162,\n  163,\n  164,\n  165,\n  166,\n  167,\n  168,\n  169,\n  170,\n  171,\n  172,\n  173,\n  174,\n  175,\n  176,\n  177,\n  178,\n  179,\n  180,\n  181,\n  182,\n  183,\n  184,\n  185,\n  186,\n  187,\n  188,\n  189,\n  190,\n  191,\n  192,\n  193,\n  194,\n  195,\n  196,\n  197,\n  198,\n  199,\n  200,\n  201,\n  202,\n  203,\n  204,\n  205,\n  206,\n  207,\n  208,\n  209,\n  210,\n  211,\n  212,\n  213,\n  214,\n  215,\n  216,\n  217,\n  218,\n  219,\n  220,\n  221,\n  222,\n  223,\n  224,\n  225,\n  226,\n  227,\n  228,\n  229,\n  230,\n  231,\n  232,\n  233,\n  234,\n  235,\n  236,\n  237,\n  238,\n  239,\n  240,\n  241,\n  242,\n  243,\n  244,\n  245,\n  246,\n  247,\n  248,\n  249,\n  250,\n  251,\n  252,\n  253,\n  254,\n  255,\n  256,\n  257,\n  258,\n  259,\n  260,\n  261,\n  262,\n  263,\n  264,\n  265,\n  266,\n  267,\n  268,\n  269,\n  270,\n  271,\n  272,\n  273,\n  274,\n  275,\n  276,\n  277,\n  278,\n  279,\n  280,\n  281,\n  282,\n  283,\n  284,\n  285,\n  286,\n  287,\n  288,\n  289,\n  290,\n  291,\n  292,\n  293,\n  294,\n  295,\n  296,\n  297,\n  298,\n  299,\n  300,\n  301,\n  302,\n  303,\n  304,\n  305,\n  306,\n  307,\n  308,\n  309,\n  310,\n  311,\n  312,\n  313,\n  314,\n  315,\n  316,\n  317,\n  318,\n  319,\n  320,\n  321,\n  322,\n  323,\n  324,\n  325,\n  326,\n  327,\n  328,\n  329,\n  330,\n  331,\n  332,\n  333,\n  334,\n  335,\n  336,\n  337,\n  338,\n  339,\n  340,\n  341,\n  342,\n  343,\n  344,\n  345,\n  346,\n  347,\n  348,\n  349,\n  350,\n  351,\n  352,\n  353,\n  354,\n  355,\n  356]]"
     },
     "metadata": {},
     "execution_count": 363
    }
   ],
   "source": [
    "list(feature_sets.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bit4539a5a599f24d04a1e10a4002ffd2cd",
   "display_name": "Python 3.8.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}