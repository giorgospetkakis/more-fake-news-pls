{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import go_to_project_root\n",
    "from scipy.stats import mode\n",
    "import data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, BernoulliRBM\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "import sklearn.svm\n",
    "import keras\n",
    "from sklearn.feature_selection import RFE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(classifier, xtrain, ytrain, xtest, ytest):\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    imp = classifier.feature_importances_\n",
    "    pred = predict(classifier, xtest)\n",
    "    return pred\n",
    "\n",
    "def feature_elim(classifier, xtrain, ytrain):\n",
    "    rfe = RFE(estimator=classifier, n_features_to_select=n_features, step=10)\n",
    "    rfe.fit(xtrain, ytrain)\n",
    "    return rfe.ranking_\n",
    "\n",
    "def get_new_xtests(path): \n",
    "    path = data_root[:-1] + \"s/\" + path\n",
    "    xtest = []\n",
    "    for file in os.listdir(path + \"X_test/\"):\n",
    "        xtest += [pd.read_csv(path + \"X_test/\" + file, index_col=0).to_numpy()[:, [2, 3, 4, 5]]]\n",
    "    return xtest\n",
    "\n",
    "def read_data(_path):\n",
    "    path = data_root + _path\n",
    "    xtrain = pd.read_csv(path + \"X_train.csv\", index_col=0).to_numpy()[:200].astype(float)\n",
    "    ytrain = pd.read_csv(path + \"y_train.csv\", index_col=0).to_numpy()[:200]\n",
    "    ytest = pd.read_csv(path + \"y_test.csv\", index_col=0).to_numpy()\n",
    "    xtest = []\n",
    "    for file in os.listdir(path + \"X_test/\"):\n",
    "        xtest += [pd.read_csv(path + \"X_test/\" + file, index_col=0).to_numpy()]\n",
    "\n",
    "    new_xt = get_new_xtests(_path)\n",
    "    mean_xt = [np.mean(xt[0], axis=0) for xt in new_xt]\n",
    "\n",
    "    for i, x in enumerate(xtest):\n",
    "        xtest[i][:,[2, 3, 4, 5]] = mean_xt[i]\n",
    "\n",
    "    return xtrain, ytrain, xtest, ytest\n",
    "\n",
    "def predict(classifier, xtest):\n",
    "    majority_vote_preds = []\n",
    "    for x in xtest:\n",
    "        x = x\n",
    "        majority_vote_preds += [np.sum(classifier.predict(x).astype(int)) > 1]\n",
    "    return majority_vote_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_to_project_root()\n",
    "data_root = \"data/processed/800/\"\n",
    "datasets = [read_data(f\"K{k+1}/\") for k in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {\n",
    "    # \"lexical\": [1],\n",
    "    \"semantic\": [2, 3, 4, 5],\n",
    "    # \"clusters\": [6, 7],\n",
    "    # \"nonling\":  list(range(8, 30)),\n",
    "    # \"pos\": [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],\n",
    "    # \"emotion\": [47, 48, 49, 50, 51, 52, 53, 54, 55, 56],\n",
    "    # \"embeddings\": list(range(57, 357))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6999999999999998\n"
    }
   ],
   "source": [
    "# ada = []\n",
    "# fs = feature_sets[\"embeddings\"] + feature_sets[\"emotion\"] + feature_sets[\"pos\"]\n",
    "_del = feature_sets[\"semantic\"]\n",
    "f = _del\n",
    "\n",
    "mean = 0\n",
    "for i in range(3):\n",
    "    xtrain, ytrain, xtest, ytest = datasets[i]\n",
    "    c = GradientBoostingClassifier()\n",
    "    # pred = classify(c, np.delete(xtrain, _del, axis=1), ytrain, xtest, ytest)\n",
    "    pred = classify(c, xtrain, ytrain, xtest, ytest)\n",
    "    acc = balanced_accuracy_score(ytest, pred)\n",
    "    mean += acc\n",
    "print(mean / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch = [x for x, p in enumerate([int(x == True) for x in pred]) if p != ytest[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0,\n 1,\n 9,\n 12,\n 13,\n 14,\n 15,\n 16,\n 22,\n 25,\n 26,\n 30,\n 33,\n 35,\n 36,\n 40,\n 41,\n 42,\n 43,\n 53,\n 58,\n 64,\n 66,\n 69,\n 70,\n 72,\n 80,\n 90,\n 93,\n 94,\n 95,\n 98]"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      0     1         2         3         4         5     6     7    8    \\\n0    7.50  0.98  0.580955  0.580955  0.580955  0.580955  0.00  0.42    1   \n1    8.56  1.00  0.619110  0.619110  0.619110  0.619110  0.02  0.44    2   \n2    8.82  1.00  0.640548  0.640548  0.640548  0.640548  0.02  0.38    2   \n3   11.14  1.00  0.648598  0.648598  0.648598  0.648598  0.12  0.32    2   \n4    9.14  0.92  0.598249  0.598249  0.598249  0.598249  0.00  0.52    1   \n5    9.68  1.00  0.579951  0.579951  0.579951  0.579951  0.08  0.52    1   \n6    6.68  1.00  0.563152  0.563152  0.563152  0.563152  0.04  0.38    1   \n7    9.66  1.00  0.597910  0.597910  0.597910  0.597910  0.08  0.58    1   \n8    8.64  1.00  0.526621  0.526621  0.526621  0.526621  0.18  0.48    2   \n9    6.18  0.98  0.630640  0.630640  0.630640  0.630640  0.08  0.34    2   \n10   9.44  1.00  0.595311  0.595311  0.595311  0.595311  0.06  0.28    1   \n11   7.14  1.00  0.681551  0.681551  0.681551  0.681551  0.00  0.32    2   \n12   8.38  0.98  0.549094  0.549094  0.549094  0.549094  0.08  0.28    2   \n13   7.62  1.00  0.609391  0.609391  0.609391  0.609391  0.04  0.30    2   \n14   7.96  1.00  0.567452  0.567452  0.567452  0.567452  0.08  0.70    2   \n15   7.06  1.00  0.627179  0.627179  0.627179  0.627179  0.00  0.72    2   \n16   6.90  1.00  0.520785  0.520785  0.520785  0.520785  0.18  0.30    2   \n17   7.40  0.94  0.588305  0.588305  0.588305  0.588305  0.08  0.20    2   \n18   7.84  1.00  0.539187  0.539187  0.539187  0.539187  0.24  0.20    2   \n19   6.68  1.00  0.591106  0.591106  0.591106  0.591106  0.02  0.34    2   \n20   5.72  0.98  0.527856  0.527856  0.527856  0.527856  0.02  0.46    2   \n21   6.20  0.90  0.623763  0.623763  0.623763  0.623763  0.02  0.26    1   \n22   8.68  0.98  0.690441  0.690441  0.690441  0.690441  0.08  0.24    1   \n23  10.00  1.00  0.577864  0.577864  0.577864  0.577864  0.00  0.72    3   \n24   6.54  0.90  0.640187  0.640187  0.640187  0.640187  0.04  0.14    2   \n25   8.34  1.00  0.551944  0.551944  0.551944  0.551944  0.06  0.30    2   \n26   7.72  1.00  0.584864  0.584864  0.584864  0.584864  0.00  0.28    2   \n27   6.70  1.00  0.471306  0.471306  0.471306  0.471306  0.00  0.14    1   \n28   7.22  1.00  0.582769  0.582769  0.582769  0.582769  0.02  0.08    2   \n29  10.34  1.00  0.565779  0.565779  0.565779  0.565779  0.02  0.42    2   \n30   8.06  1.00  0.530564  0.530564  0.530564  0.530564  0.02  0.36    2   \n31   5.28  1.00  0.541258  0.541258  0.541258  0.541258  0.00  0.18    2   \n\n     9    ...       347       348       349       350       351       352  \\\n0   1.00  ...  0.284624 -0.013412 -0.195229  0.184144  0.188326 -0.020228   \n1   1.56  ...  0.260419 -0.010028 -0.174950  0.140882  0.109642  0.052822   \n2   1.96  ...  0.198711 -0.000020 -0.145692  0.158541  0.117234  0.100025   \n3   1.06  ...  0.209349  0.012015 -0.176687  0.286394  0.081167  0.059793   \n4   1.00  ...  0.281274 -0.022952 -0.132948  0.210196  0.102468  0.023697   \n5   1.00  ...  0.288692 -0.031779 -0.160704  0.212836  0.178110  0.068823   \n6   1.00  ...  0.282537  0.003542 -0.088421  0.117531  0.100682  0.071690   \n7   1.00  ...  0.283849 -0.040608 -0.153684  0.196859  0.214526  0.037496   \n8   1.02  ...  0.227369  0.002175 -0.141336  0.102787  0.127321  0.059000   \n9   0.92  ...  0.283571  0.020827 -0.149753  0.219545  0.085846  0.129766   \n10  1.00  ...  0.199048 -0.020359 -0.111508  0.193598  0.116383  0.080548   \n11  2.00  ...  0.268049  0.010376 -0.059886  0.197630  0.046856  0.046871   \n12  1.02  ...  0.216314  0.003231 -0.107506  0.173707  0.108290  0.073758   \n13  1.76  ...  0.284097  0.012260 -0.092296  0.161234  0.134799  0.031847   \n14  1.68  ...  0.212923  0.051712 -0.108884  0.161840  0.117847  0.047866   \n15  1.84  ...  0.254418 -0.065668 -0.144483  0.204487  0.166599  0.025474   \n16  0.96  ...  0.229845  0.006435 -0.093976  0.144969  0.111061  0.084076   \n17  1.80  ...  0.222071 -0.012069 -0.102431  0.134223  0.151856  0.058861   \n18  1.24  ...  0.232183  0.044526 -0.243043  0.236431  0.096910  0.061974   \n19  0.70  ...  0.211974 -0.034502 -0.107806  0.134025  0.031485  0.132725   \n20  1.92  ...  0.247103 -0.020387 -0.091617  0.151275  0.163580  0.004306   \n21  1.00  ...  0.274672  0.010029 -0.163421  0.142062  0.155316  0.030203   \n22  1.00  ...  0.324256 -0.038353 -0.176213  0.181269  0.150709 -0.000456   \n23  2.00  ...  0.169335 -0.016316 -0.164363  0.180981  0.121270  0.064859   \n24  1.98  ...  0.284313 -0.055719 -0.115375  0.142465  0.085027  0.009986   \n25  0.38  ...  0.248382  0.000612 -0.135905  0.210191  0.069763  0.089686   \n26  1.30  ...  0.286252  0.017708 -0.153523  0.180665  0.119512  0.023961   \n27  0.94  ...  0.295409 -0.034407 -0.096428  0.168383  0.064764 -0.008872   \n28  1.00  ...  0.237735  0.053310 -0.094034  0.074490  0.106513  0.074977   \n29  1.04  ...  0.251318 -0.028887 -0.100963  0.110775  0.062870  0.080216   \n30  1.24  ...  0.208101 -0.012976 -0.101445  0.178550  0.122885  0.045357   \n31  1.02  ...  0.265922  0.005143 -0.095884  0.162614  0.190332  0.003982   \n\n         353       354       355       356  \n0   0.040004 -0.008719 -0.048525  0.054909  \n1   0.011637 -0.034743 -0.029555  0.092662  \n2  -0.045165  0.055751 -0.059186  0.031795  \n3  -0.025541  0.060909 -0.089637  0.010788  \n4   0.003421 -0.011731 -0.110694  0.041663  \n5   0.014393 -0.012220 -0.123156  0.017638  \n6  -0.022632  0.019563 -0.076970  0.059577  \n7   0.033136 -0.019455 -0.075675  0.037446  \n8  -0.037056  0.006883 -0.060917  0.081950  \n9  -0.022316  0.037707 -0.102794 -0.003213  \n10 -0.029447  0.064245 -0.042525  0.020361  \n11  0.009483 -0.026331 -0.024754  0.073309  \n12 -0.039887  0.040109 -0.050138  0.055940  \n13 -0.034458  0.062262 -0.084808  0.093944  \n14 -0.034609  0.068680 -0.021110  0.088822  \n15 -0.009587 -0.055404 -0.097356  0.100250  \n16 -0.042976  0.055162 -0.051970  0.048090  \n17 -0.052971  0.067680 -0.039791  0.041178  \n18 -0.010752  0.055054 -0.079126 -0.005435  \n19 -0.046978  0.006063 -0.079127  0.046315  \n20 -0.026290  0.026448 -0.089248  0.044186  \n21 -0.020394  0.018147 -0.064043  0.068041  \n22  0.068578 -0.014583 -0.097392 -0.022784  \n23 -0.056053  0.030129 -0.025244  0.083364  \n24  0.018053 -0.023819 -0.086216  0.092329  \n25 -0.029078  0.091382 -0.074203 -0.016367  \n26 -0.043953  0.051033 -0.080711  0.042857  \n27  0.005062 -0.013285 -0.070194  0.086155  \n28  0.005042  0.064890 -0.048052  0.043263  \n29 -0.049728  0.017559 -0.054494  0.078599  \n30 -0.034603  0.056281 -0.088895  0.027867  \n31 -0.011848  0.016757 -0.076880  0.104585  \n\n[32 rows x 357 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>347</th>\n      <th>348</th>\n      <th>349</th>\n      <th>350</th>\n      <th>351</th>\n      <th>352</th>\n      <th>353</th>\n      <th>354</th>\n      <th>355</th>\n      <th>356</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.50</td>\n      <td>0.98</td>\n      <td>0.580955</td>\n      <td>0.580955</td>\n      <td>0.580955</td>\n      <td>0.580955</td>\n      <td>0.00</td>\n      <td>0.42</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.284624</td>\n      <td>-0.013412</td>\n      <td>-0.195229</td>\n      <td>0.184144</td>\n      <td>0.188326</td>\n      <td>-0.020228</td>\n      <td>0.040004</td>\n      <td>-0.008719</td>\n      <td>-0.048525</td>\n      <td>0.054909</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.56</td>\n      <td>1.00</td>\n      <td>0.619110</td>\n      <td>0.619110</td>\n      <td>0.619110</td>\n      <td>0.619110</td>\n      <td>0.02</td>\n      <td>0.44</td>\n      <td>2</td>\n      <td>1.56</td>\n      <td>...</td>\n      <td>0.260419</td>\n      <td>-0.010028</td>\n      <td>-0.174950</td>\n      <td>0.140882</td>\n      <td>0.109642</td>\n      <td>0.052822</td>\n      <td>0.011637</td>\n      <td>-0.034743</td>\n      <td>-0.029555</td>\n      <td>0.092662</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.82</td>\n      <td>1.00</td>\n      <td>0.640548</td>\n      <td>0.640548</td>\n      <td>0.640548</td>\n      <td>0.640548</td>\n      <td>0.02</td>\n      <td>0.38</td>\n      <td>2</td>\n      <td>1.96</td>\n      <td>...</td>\n      <td>0.198711</td>\n      <td>-0.000020</td>\n      <td>-0.145692</td>\n      <td>0.158541</td>\n      <td>0.117234</td>\n      <td>0.100025</td>\n      <td>-0.045165</td>\n      <td>0.055751</td>\n      <td>-0.059186</td>\n      <td>0.031795</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.14</td>\n      <td>1.00</td>\n      <td>0.648598</td>\n      <td>0.648598</td>\n      <td>0.648598</td>\n      <td>0.648598</td>\n      <td>0.12</td>\n      <td>0.32</td>\n      <td>2</td>\n      <td>1.06</td>\n      <td>...</td>\n      <td>0.209349</td>\n      <td>0.012015</td>\n      <td>-0.176687</td>\n      <td>0.286394</td>\n      <td>0.081167</td>\n      <td>0.059793</td>\n      <td>-0.025541</td>\n      <td>0.060909</td>\n      <td>-0.089637</td>\n      <td>0.010788</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9.14</td>\n      <td>0.92</td>\n      <td>0.598249</td>\n      <td>0.598249</td>\n      <td>0.598249</td>\n      <td>0.598249</td>\n      <td>0.00</td>\n      <td>0.52</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.281274</td>\n      <td>-0.022952</td>\n      <td>-0.132948</td>\n      <td>0.210196</td>\n      <td>0.102468</td>\n      <td>0.023697</td>\n      <td>0.003421</td>\n      <td>-0.011731</td>\n      <td>-0.110694</td>\n      <td>0.041663</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>9.68</td>\n      <td>1.00</td>\n      <td>0.579951</td>\n      <td>0.579951</td>\n      <td>0.579951</td>\n      <td>0.579951</td>\n      <td>0.08</td>\n      <td>0.52</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.288692</td>\n      <td>-0.031779</td>\n      <td>-0.160704</td>\n      <td>0.212836</td>\n      <td>0.178110</td>\n      <td>0.068823</td>\n      <td>0.014393</td>\n      <td>-0.012220</td>\n      <td>-0.123156</td>\n      <td>0.017638</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6.68</td>\n      <td>1.00</td>\n      <td>0.563152</td>\n      <td>0.563152</td>\n      <td>0.563152</td>\n      <td>0.563152</td>\n      <td>0.04</td>\n      <td>0.38</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.282537</td>\n      <td>0.003542</td>\n      <td>-0.088421</td>\n      <td>0.117531</td>\n      <td>0.100682</td>\n      <td>0.071690</td>\n      <td>-0.022632</td>\n      <td>0.019563</td>\n      <td>-0.076970</td>\n      <td>0.059577</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>9.66</td>\n      <td>1.00</td>\n      <td>0.597910</td>\n      <td>0.597910</td>\n      <td>0.597910</td>\n      <td>0.597910</td>\n      <td>0.08</td>\n      <td>0.58</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.283849</td>\n      <td>-0.040608</td>\n      <td>-0.153684</td>\n      <td>0.196859</td>\n      <td>0.214526</td>\n      <td>0.037496</td>\n      <td>0.033136</td>\n      <td>-0.019455</td>\n      <td>-0.075675</td>\n      <td>0.037446</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8.64</td>\n      <td>1.00</td>\n      <td>0.526621</td>\n      <td>0.526621</td>\n      <td>0.526621</td>\n      <td>0.526621</td>\n      <td>0.18</td>\n      <td>0.48</td>\n      <td>2</td>\n      <td>1.02</td>\n      <td>...</td>\n      <td>0.227369</td>\n      <td>0.002175</td>\n      <td>-0.141336</td>\n      <td>0.102787</td>\n      <td>0.127321</td>\n      <td>0.059000</td>\n      <td>-0.037056</td>\n      <td>0.006883</td>\n      <td>-0.060917</td>\n      <td>0.081950</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>6.18</td>\n      <td>0.98</td>\n      <td>0.630640</td>\n      <td>0.630640</td>\n      <td>0.630640</td>\n      <td>0.630640</td>\n      <td>0.08</td>\n      <td>0.34</td>\n      <td>2</td>\n      <td>0.92</td>\n      <td>...</td>\n      <td>0.283571</td>\n      <td>0.020827</td>\n      <td>-0.149753</td>\n      <td>0.219545</td>\n      <td>0.085846</td>\n      <td>0.129766</td>\n      <td>-0.022316</td>\n      <td>0.037707</td>\n      <td>-0.102794</td>\n      <td>-0.003213</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>9.44</td>\n      <td>1.00</td>\n      <td>0.595311</td>\n      <td>0.595311</td>\n      <td>0.595311</td>\n      <td>0.595311</td>\n      <td>0.06</td>\n      <td>0.28</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.199048</td>\n      <td>-0.020359</td>\n      <td>-0.111508</td>\n      <td>0.193598</td>\n      <td>0.116383</td>\n      <td>0.080548</td>\n      <td>-0.029447</td>\n      <td>0.064245</td>\n      <td>-0.042525</td>\n      <td>0.020361</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>7.14</td>\n      <td>1.00</td>\n      <td>0.681551</td>\n      <td>0.681551</td>\n      <td>0.681551</td>\n      <td>0.681551</td>\n      <td>0.00</td>\n      <td>0.32</td>\n      <td>2</td>\n      <td>2.00</td>\n      <td>...</td>\n      <td>0.268049</td>\n      <td>0.010376</td>\n      <td>-0.059886</td>\n      <td>0.197630</td>\n      <td>0.046856</td>\n      <td>0.046871</td>\n      <td>0.009483</td>\n      <td>-0.026331</td>\n      <td>-0.024754</td>\n      <td>0.073309</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>8.38</td>\n      <td>0.98</td>\n      <td>0.549094</td>\n      <td>0.549094</td>\n      <td>0.549094</td>\n      <td>0.549094</td>\n      <td>0.08</td>\n      <td>0.28</td>\n      <td>2</td>\n      <td>1.02</td>\n      <td>...</td>\n      <td>0.216314</td>\n      <td>0.003231</td>\n      <td>-0.107506</td>\n      <td>0.173707</td>\n      <td>0.108290</td>\n      <td>0.073758</td>\n      <td>-0.039887</td>\n      <td>0.040109</td>\n      <td>-0.050138</td>\n      <td>0.055940</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>7.62</td>\n      <td>1.00</td>\n      <td>0.609391</td>\n      <td>0.609391</td>\n      <td>0.609391</td>\n      <td>0.609391</td>\n      <td>0.04</td>\n      <td>0.30</td>\n      <td>2</td>\n      <td>1.76</td>\n      <td>...</td>\n      <td>0.284097</td>\n      <td>0.012260</td>\n      <td>-0.092296</td>\n      <td>0.161234</td>\n      <td>0.134799</td>\n      <td>0.031847</td>\n      <td>-0.034458</td>\n      <td>0.062262</td>\n      <td>-0.084808</td>\n      <td>0.093944</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>7.96</td>\n      <td>1.00</td>\n      <td>0.567452</td>\n      <td>0.567452</td>\n      <td>0.567452</td>\n      <td>0.567452</td>\n      <td>0.08</td>\n      <td>0.70</td>\n      <td>2</td>\n      <td>1.68</td>\n      <td>...</td>\n      <td>0.212923</td>\n      <td>0.051712</td>\n      <td>-0.108884</td>\n      <td>0.161840</td>\n      <td>0.117847</td>\n      <td>0.047866</td>\n      <td>-0.034609</td>\n      <td>0.068680</td>\n      <td>-0.021110</td>\n      <td>0.088822</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>7.06</td>\n      <td>1.00</td>\n      <td>0.627179</td>\n      <td>0.627179</td>\n      <td>0.627179</td>\n      <td>0.627179</td>\n      <td>0.00</td>\n      <td>0.72</td>\n      <td>2</td>\n      <td>1.84</td>\n      <td>...</td>\n      <td>0.254418</td>\n      <td>-0.065668</td>\n      <td>-0.144483</td>\n      <td>0.204487</td>\n      <td>0.166599</td>\n      <td>0.025474</td>\n      <td>-0.009587</td>\n      <td>-0.055404</td>\n      <td>-0.097356</td>\n      <td>0.100250</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>6.90</td>\n      <td>1.00</td>\n      <td>0.520785</td>\n      <td>0.520785</td>\n      <td>0.520785</td>\n      <td>0.520785</td>\n      <td>0.18</td>\n      <td>0.30</td>\n      <td>2</td>\n      <td>0.96</td>\n      <td>...</td>\n      <td>0.229845</td>\n      <td>0.006435</td>\n      <td>-0.093976</td>\n      <td>0.144969</td>\n      <td>0.111061</td>\n      <td>0.084076</td>\n      <td>-0.042976</td>\n      <td>0.055162</td>\n      <td>-0.051970</td>\n      <td>0.048090</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>7.40</td>\n      <td>0.94</td>\n      <td>0.588305</td>\n      <td>0.588305</td>\n      <td>0.588305</td>\n      <td>0.588305</td>\n      <td>0.08</td>\n      <td>0.20</td>\n      <td>2</td>\n      <td>1.80</td>\n      <td>...</td>\n      <td>0.222071</td>\n      <td>-0.012069</td>\n      <td>-0.102431</td>\n      <td>0.134223</td>\n      <td>0.151856</td>\n      <td>0.058861</td>\n      <td>-0.052971</td>\n      <td>0.067680</td>\n      <td>-0.039791</td>\n      <td>0.041178</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>7.84</td>\n      <td>1.00</td>\n      <td>0.539187</td>\n      <td>0.539187</td>\n      <td>0.539187</td>\n      <td>0.539187</td>\n      <td>0.24</td>\n      <td>0.20</td>\n      <td>2</td>\n      <td>1.24</td>\n      <td>...</td>\n      <td>0.232183</td>\n      <td>0.044526</td>\n      <td>-0.243043</td>\n      <td>0.236431</td>\n      <td>0.096910</td>\n      <td>0.061974</td>\n      <td>-0.010752</td>\n      <td>0.055054</td>\n      <td>-0.079126</td>\n      <td>-0.005435</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>6.68</td>\n      <td>1.00</td>\n      <td>0.591106</td>\n      <td>0.591106</td>\n      <td>0.591106</td>\n      <td>0.591106</td>\n      <td>0.02</td>\n      <td>0.34</td>\n      <td>2</td>\n      <td>0.70</td>\n      <td>...</td>\n      <td>0.211974</td>\n      <td>-0.034502</td>\n      <td>-0.107806</td>\n      <td>0.134025</td>\n      <td>0.031485</td>\n      <td>0.132725</td>\n      <td>-0.046978</td>\n      <td>0.006063</td>\n      <td>-0.079127</td>\n      <td>0.046315</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>5.72</td>\n      <td>0.98</td>\n      <td>0.527856</td>\n      <td>0.527856</td>\n      <td>0.527856</td>\n      <td>0.527856</td>\n      <td>0.02</td>\n      <td>0.46</td>\n      <td>2</td>\n      <td>1.92</td>\n      <td>...</td>\n      <td>0.247103</td>\n      <td>-0.020387</td>\n      <td>-0.091617</td>\n      <td>0.151275</td>\n      <td>0.163580</td>\n      <td>0.004306</td>\n      <td>-0.026290</td>\n      <td>0.026448</td>\n      <td>-0.089248</td>\n      <td>0.044186</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>6.20</td>\n      <td>0.90</td>\n      <td>0.623763</td>\n      <td>0.623763</td>\n      <td>0.623763</td>\n      <td>0.623763</td>\n      <td>0.02</td>\n      <td>0.26</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.274672</td>\n      <td>0.010029</td>\n      <td>-0.163421</td>\n      <td>0.142062</td>\n      <td>0.155316</td>\n      <td>0.030203</td>\n      <td>-0.020394</td>\n      <td>0.018147</td>\n      <td>-0.064043</td>\n      <td>0.068041</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>8.68</td>\n      <td>0.98</td>\n      <td>0.690441</td>\n      <td>0.690441</td>\n      <td>0.690441</td>\n      <td>0.690441</td>\n      <td>0.08</td>\n      <td>0.24</td>\n      <td>1</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.324256</td>\n      <td>-0.038353</td>\n      <td>-0.176213</td>\n      <td>0.181269</td>\n      <td>0.150709</td>\n      <td>-0.000456</td>\n      <td>0.068578</td>\n      <td>-0.014583</td>\n      <td>-0.097392</td>\n      <td>-0.022784</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>10.00</td>\n      <td>1.00</td>\n      <td>0.577864</td>\n      <td>0.577864</td>\n      <td>0.577864</td>\n      <td>0.577864</td>\n      <td>0.00</td>\n      <td>0.72</td>\n      <td>3</td>\n      <td>2.00</td>\n      <td>...</td>\n      <td>0.169335</td>\n      <td>-0.016316</td>\n      <td>-0.164363</td>\n      <td>0.180981</td>\n      <td>0.121270</td>\n      <td>0.064859</td>\n      <td>-0.056053</td>\n      <td>0.030129</td>\n      <td>-0.025244</td>\n      <td>0.083364</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>6.54</td>\n      <td>0.90</td>\n      <td>0.640187</td>\n      <td>0.640187</td>\n      <td>0.640187</td>\n      <td>0.640187</td>\n      <td>0.04</td>\n      <td>0.14</td>\n      <td>2</td>\n      <td>1.98</td>\n      <td>...</td>\n      <td>0.284313</td>\n      <td>-0.055719</td>\n      <td>-0.115375</td>\n      <td>0.142465</td>\n      <td>0.085027</td>\n      <td>0.009986</td>\n      <td>0.018053</td>\n      <td>-0.023819</td>\n      <td>-0.086216</td>\n      <td>0.092329</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>8.34</td>\n      <td>1.00</td>\n      <td>0.551944</td>\n      <td>0.551944</td>\n      <td>0.551944</td>\n      <td>0.551944</td>\n      <td>0.06</td>\n      <td>0.30</td>\n      <td>2</td>\n      <td>0.38</td>\n      <td>...</td>\n      <td>0.248382</td>\n      <td>0.000612</td>\n      <td>-0.135905</td>\n      <td>0.210191</td>\n      <td>0.069763</td>\n      <td>0.089686</td>\n      <td>-0.029078</td>\n      <td>0.091382</td>\n      <td>-0.074203</td>\n      <td>-0.016367</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>7.72</td>\n      <td>1.00</td>\n      <td>0.584864</td>\n      <td>0.584864</td>\n      <td>0.584864</td>\n      <td>0.584864</td>\n      <td>0.00</td>\n      <td>0.28</td>\n      <td>2</td>\n      <td>1.30</td>\n      <td>...</td>\n      <td>0.286252</td>\n      <td>0.017708</td>\n      <td>-0.153523</td>\n      <td>0.180665</td>\n      <td>0.119512</td>\n      <td>0.023961</td>\n      <td>-0.043953</td>\n      <td>0.051033</td>\n      <td>-0.080711</td>\n      <td>0.042857</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>6.70</td>\n      <td>1.00</td>\n      <td>0.471306</td>\n      <td>0.471306</td>\n      <td>0.471306</td>\n      <td>0.471306</td>\n      <td>0.00</td>\n      <td>0.14</td>\n      <td>1</td>\n      <td>0.94</td>\n      <td>...</td>\n      <td>0.295409</td>\n      <td>-0.034407</td>\n      <td>-0.096428</td>\n      <td>0.168383</td>\n      <td>0.064764</td>\n      <td>-0.008872</td>\n      <td>0.005062</td>\n      <td>-0.013285</td>\n      <td>-0.070194</td>\n      <td>0.086155</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>7.22</td>\n      <td>1.00</td>\n      <td>0.582769</td>\n      <td>0.582769</td>\n      <td>0.582769</td>\n      <td>0.582769</td>\n      <td>0.02</td>\n      <td>0.08</td>\n      <td>2</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>0.237735</td>\n      <td>0.053310</td>\n      <td>-0.094034</td>\n      <td>0.074490</td>\n      <td>0.106513</td>\n      <td>0.074977</td>\n      <td>0.005042</td>\n      <td>0.064890</td>\n      <td>-0.048052</td>\n      <td>0.043263</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>10.34</td>\n      <td>1.00</td>\n      <td>0.565779</td>\n      <td>0.565779</td>\n      <td>0.565779</td>\n      <td>0.565779</td>\n      <td>0.02</td>\n      <td>0.42</td>\n      <td>2</td>\n      <td>1.04</td>\n      <td>...</td>\n      <td>0.251318</td>\n      <td>-0.028887</td>\n      <td>-0.100963</td>\n      <td>0.110775</td>\n      <td>0.062870</td>\n      <td>0.080216</td>\n      <td>-0.049728</td>\n      <td>0.017559</td>\n      <td>-0.054494</td>\n      <td>0.078599</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>8.06</td>\n      <td>1.00</td>\n      <td>0.530564</td>\n      <td>0.530564</td>\n      <td>0.530564</td>\n      <td>0.530564</td>\n      <td>0.02</td>\n      <td>0.36</td>\n      <td>2</td>\n      <td>1.24</td>\n      <td>...</td>\n      <td>0.208101</td>\n      <td>-0.012976</td>\n      <td>-0.101445</td>\n      <td>0.178550</td>\n      <td>0.122885</td>\n      <td>0.045357</td>\n      <td>-0.034603</td>\n      <td>0.056281</td>\n      <td>-0.088895</td>\n      <td>0.027867</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>5.28</td>\n      <td>1.00</td>\n      <td>0.541258</td>\n      <td>0.541258</td>\n      <td>0.541258</td>\n      <td>0.541258</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>2</td>\n      <td>1.02</td>\n      <td>...</td>\n      <td>0.265922</td>\n      <td>0.005143</td>\n      <td>-0.095884</td>\n      <td>0.162614</td>\n      <td>0.190332</td>\n      <td>0.003982</td>\n      <td>-0.011848</td>\n      <td>0.016757</td>\n      <td>-0.076880</td>\n      <td>0.104585</td>\n    </tr>\n  </tbody>\n</table>\n<p>32 rows × 357 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "pd.DataFrame([xtest[m][0] for m in mismatch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'extra' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b681375bbc66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Extra Trees\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Logistic Regression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandomf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Random Forest\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# plt.plot(range(5, 300, 5), xgboost, label=\"XGBoost\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extra' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(range(5, 300, 5), extra, label=\"Extra Trees\")\n",
    "plt.plot(range(5, 300, 5), logs, label=\"Logistic Regression\")\n",
    "plt.plot(range(5, 300, 5), randomf, label=\"Random Forest\")\n",
    "# plt.plot(range(5, 300, 5), xgboost, label=\"XGBoost\")\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel(\"Number of Selected Features\", fontsize=20)\n",
    "plt.ylabel(\"Testing accuracy\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2.0300000000000002"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "0.68 + 0.73 + 0.6200000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 353 and input n_features is 357 ",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-76899337c630>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-bc5c631a5014>\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(classifier, xtrain, ytrain, xtest, ytest)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mimp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-bc5c631a5014>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(classifier, xtest)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxtest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mmajority_vote_preds\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmajority_vote_preds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \"\"\"\n\u001b[1;32m-> 1172\u001b[1;33m         \u001b[0mraw_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1173\u001b[0m         \u001b[0mencoded_labels\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_prediction_to_decision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1126\u001b[0m         \"\"\"\n\u001b[0;32m   1127\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1128\u001b[1;33m         \u001b[0mraw_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_raw_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_raw_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[1;34m\"\"\"Return the sum of the trees raw predictions (+ init estimator).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m         \u001b[0mraw_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_predict_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m         predict_stages(self.estimators_, X, self.learning_rate,\n\u001b[0;32m    620\u001b[0m                        raw_predictions)\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_raw_predict_init\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;34m\"\"\"Check input and compute raw predictions of the init estimator.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m             raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m             raise ValueError(\"Number of features of the model must \"\n\u001b[0m\u001b[0;32m    397\u001b[0m                              \u001b[1;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m                              \u001b[1;34m\"input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 353 and input n_features is 357 "
     ]
    }
   ],
   "source": [
    "f = _del\n",
    "for i in range(3):\n",
    "    xtrain, ytrain, xtest, ytest = datasets[i]\n",
    "    mean, _  = classify(c, np.delete(xtrain, f, axis=1), ytrain, xtest, ytest)\n",
    "    print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[2, 3, 4, 5]]"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "list(feature_sets.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bit4539a5a599f24d04a1e10a4002ffd2cd",
   "display_name": "Python 3.8.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}