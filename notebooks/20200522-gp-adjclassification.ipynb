{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import data\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'many  rare  prosecutorial  full  relevant  former former former  legal  ethical  only  cheap  2nd  fair  public public  retired  obvious  inadmissible  real  wrongful  vulgarian  wrong  latina  worst  evil'"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "stoplist = set(stopwords.words('english'))\n",
    "\n",
    "authors = data.get_processed_data()\n",
    "\n",
    "truths = [auth.truth for auth in list(authors.values())]\n",
    "adjectives = [auth.adjectives for auth in list(authors.values())]\n",
    "\n",
    "adj_str = []\n",
    "for adj in adjectives:\n",
    "    _str = \" \"\n",
    "    for text, count in adj.items():\n",
    "        _str += \" \" + (text + \" \") * count\n",
    "    adj_str += [_str.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mcts_adj(authors, n_models=50, k=3, threshold=1.0, _max_iter=5000, _n_init=5, data_split=0.6667):\n",
    "    '''\n",
    "    Extract the most common terms used by fake-news spreaders for the given authors.\n",
    "    Trains an n_models number of models and takes the intersection of the most common terms used by fake news spreaders.\n",
    "    Uses k-means and tf-idf\n",
    "\n",
    "    Parameters:\n",
    "        authors(Author dict):\n",
    "            The authors to calculate the most common terms for\n",
    "\n",
    "        n_models(int):\n",
    "            The number of models to train\n",
    "            Default is 50.\n",
    "\n",
    "        k(int):\n",
    "            The number of clusters for each model\n",
    "            Default is 3\n",
    "\n",
    "        _max_iter(int):\n",
    "            The maximum number of iterations for each model\n",
    "            Default is 5000\n",
    "\n",
    "        _n_init(int):\n",
    "            The number of initializations the k-means model will do. Returns the best one.\n",
    "            Default is 5\n",
    "\n",
    "        data_split(float):\n",
    "            The splitting point between the testing set and training set for the data.\n",
    "            Default is 60%\n",
    "    '''\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "    # Get truth values\n",
    "    truths = [auth.truth for auth in list(authors.values())]\n",
    "    # Get ADJ values \n",
    "    adjectives = [auth.adjectives for auth in list(authors.values())]\n",
    "\n",
    "    tweets = []\n",
    "    for adj in adjectives:\n",
    "        _str = \" \"\n",
    "        for text, count in adj.items():\n",
    "            _str += \" \" + (text + \" \") * count\n",
    "        tweets += [_str.strip()]\n",
    "\n",
    "    # This will be the list of sets of most common adjectives used by fake news spreaders\n",
    "    list_of_sets = []\n",
    "\n",
    "    # The split of training vs. testing data\n",
    "    split = int(len(tweets) * data_split)\n",
    "    tweetsTrain = tweets[:split]\n",
    "    tweetsTest = tweets[split:]\n",
    "\n",
    "    print(\"Creating models...\")\n",
    "    while len(list_of_sets) < n_models:\n",
    "\n",
    "        # Create model from the vectorizer\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        X = vectorizer.fit_transform(tweetsTrain)\n",
    "        model = KMeans(n_clusters=k, algorithm=\"full\", init='k-means++', max_iter=_max_iter, n_init=_n_init)\n",
    "        model.fit(X)\n",
    "\n",
    "        # Order the centroids \n",
    "        order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "        terms = vectorizer.get_feature_names()\n",
    "\n",
    "        # Generate predictions based on the test set\n",
    "        pred = np.zeros((len(tweetsTest), 2))\n",
    "        for i, tw in enumerate(tweetsTest):\n",
    "            X = vectorizer.transform([tw])\n",
    "            predicted = model.predict(X)\n",
    "            pred[i] = [int(predicted), truths[i + split]]\n",
    "\n",
    "        # Fetch predictions results\n",
    "        res = [(pred[np.where(pred[:, 1]==k)])[:,0] for k in range(2)]\n",
    "        \n",
    "        # Calculate the cluster with the maximum purity\n",
    "        max_purity = 0 ; pure_cluster = -1\n",
    "        for c in range(k):\n",
    "            class_a = len(np.where(res[0]==c)[0])\n",
    "            class_b = len(np.where(res[1]==c)[0])\n",
    "\n",
    "            class_sum = class_a + class_b\n",
    "            if class_sum != 0:\n",
    "                purity = abs(0.5 - class_a / class_sum) * (class_sum/len(tweetsTest)) * 10\n",
    "            else:\n",
    "                purity = 0\n",
    "            if purity > max_purity:\n",
    "                pure_cluster = c\n",
    "                max_purity = purity\n",
    "\n",
    "        # Check if there is a cluster with very high purity\n",
    "        if max_purity >= threshold:\n",
    "            list_of_sets += [set()]\n",
    "            if len(list_of_sets) % (n_models / 4) == 0 and len(list_of_sets) > 0:\n",
    "                print(f\"{len(list_of_sets) / (n_models) * 100}% of requested models trained\")\n",
    "            for ind in order_centroids[pure_cluster,:100]:\n",
    "                # Add it to the list of sets\n",
    "                list_of_sets[-1].add(terms[ind])\n",
    "    \n",
    "    # Calculate the final set as an intersection of all sets identified\n",
    "    final_set = list_of_sets[0]\n",
    "    for s in list_of_sets:\n",
    "        final_set = final_set.intersection(s)\n",
    "\n",
    "    for author in authors.keys():\n",
    "        # Get cleaned values\n",
    "        cleaned = \" \".join(authors[author].clean)\n",
    "        count = 0\n",
    "\n",
    "        # Count the number of terms in each author\n",
    "        for term in list(final_set):\n",
    "            count += cleaned.count(re.sub(\"_\", \" \", term))\n",
    "\n",
    "        # Save to author\n",
    "        authors[author].most_common_adj_score = count\n",
    "\n",
    "    return authors, final_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = set(stopwords.words('english'))\n",
    "\n",
    "authors = data.get_processed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading data...\nCreating models...\n25.0% of requested models trained\n50.0% of requested models trained\n75.0% of requested models trained\n100.0% of requested models trained\n"
    }
   ],
   "source": [
    "authors, final_set = extract_mcts_adj(authors, n_models=100, _max_iter=9999, _n_init=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'american',\n 'best',\n 'big',\n 'black',\n 'dead',\n 'democratic',\n 'female',\n 'free',\n 'illegal',\n 'little',\n 'massive',\n 'new',\n 'old',\n 'political',\n 'red',\n 'right',\n 'true',\n 'trump'}"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "final_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in authors.values():\n",
    "    data.exportJSON(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     max_similar  min_similar  mean_similar  number_identical  mcts_ner  \\\n0       0.996152     0.554930      0.772765               0.0      28.0   \n1       0.999985     0.525862      0.827712             565.0       1.0   \n2       0.995010     0.432037      0.846995               2.0       3.0   \n3       0.942972     0.474534      0.784161               6.0       5.0   \n4       0.994065     0.459044      0.807674               0.0       7.0   \n..           ...          ...           ...               ...       ...   \n295     0.997494     0.811975      0.910553             327.0      17.0   \n296     0.930048     0.592229      0.789775              72.0       0.0   \n297     0.995494     0.253093      0.821176               0.0      43.0   \n298     0.981666     0.806598      0.932583             447.0       9.0   \n299     0.986748     0.588679      0.833404               0.0       2.0   \n\n     mcts_adj  url_max  url_mean  hashtag_max  hashtag_mean  ...  PART  PRON  \\\n0        41.0      0.0      0.00          0.0          0.00  ...  0.28  0.52   \n1         2.0      1.0      0.55          0.0          0.00  ...  0.14  0.27   \n2        72.0      3.0      2.00          4.0          0.74  ...  0.27  0.09   \n3        28.0      1.0      1.00          0.0          0.00  ...  0.51  0.35   \n4        69.0      2.0      1.30          1.0          0.02  ...  0.37  0.28   \n..        ...      ...       ...          ...           ...  ...   ...   ...   \n295      34.0      2.0      1.49          3.0          2.53  ...  0.34  0.04   \n296      18.0      1.0      1.00          2.0          0.06  ...  0.33  0.37   \n297      29.0      1.0      0.33          2.0          0.06  ...  0.54  1.31   \n298       5.0      2.0      1.87          3.0          2.56  ...  0.18  0.07   \n299       9.0      2.0      1.60          1.0          0.01  ...  0.20  0.42   \n\n     PROPN  PUNCT  SCONJ   SYM  VERB     X  TOKEN  truth  \n0     2.52   0.73   0.28  0.03  2.17  0.00  13.60    1.0  \n1     1.05   1.85   0.09  0.04  0.56  0.00  12.10    0.0  \n2     4.21   2.43   0.01  0.01  1.00  0.00  13.38    1.0  \n3     5.47   0.82   0.02  0.02  1.36  0.02  13.55    1.0  \n4     2.20   1.48   0.19  0.00  1.74  0.00  14.32    0.0  \n..     ...    ...    ...   ...   ...   ...    ...    ...  \n295   2.94   1.85   0.12  0.00  1.51  0.04  13.19    0.0  \n296   2.03   1.27   0.11  0.03  1.23  0.00  12.65    1.0  \n297   1.55   2.53   0.20  0.07  2.36  0.06  19.26    1.0  \n298   4.11   0.76   0.02  0.02  1.15  0.00   9.01    0.0  \n299   1.17   0.53   0.07  0.00  1.06  0.00  10.00    1.0  \n\n[300 rows x 47 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>max_similar</th>\n      <th>min_similar</th>\n      <th>mean_similar</th>\n      <th>number_identical</th>\n      <th>mcts_ner</th>\n      <th>mcts_adj</th>\n      <th>url_max</th>\n      <th>url_mean</th>\n      <th>hashtag_max</th>\n      <th>hashtag_mean</th>\n      <th>...</th>\n      <th>PART</th>\n      <th>PRON</th>\n      <th>PROPN</th>\n      <th>PUNCT</th>\n      <th>SCONJ</th>\n      <th>SYM</th>\n      <th>VERB</th>\n      <th>X</th>\n      <th>TOKEN</th>\n      <th>truth</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.996152</td>\n      <td>0.554930</td>\n      <td>0.772765</td>\n      <td>0.0</td>\n      <td>28.0</td>\n      <td>41.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.28</td>\n      <td>0.52</td>\n      <td>2.52</td>\n      <td>0.73</td>\n      <td>0.28</td>\n      <td>0.03</td>\n      <td>2.17</td>\n      <td>0.00</td>\n      <td>13.60</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.999985</td>\n      <td>0.525862</td>\n      <td>0.827712</td>\n      <td>565.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.55</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.14</td>\n      <td>0.27</td>\n      <td>1.05</td>\n      <td>1.85</td>\n      <td>0.09</td>\n      <td>0.04</td>\n      <td>0.56</td>\n      <td>0.00</td>\n      <td>12.10</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.995010</td>\n      <td>0.432037</td>\n      <td>0.846995</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>72.0</td>\n      <td>3.0</td>\n      <td>2.00</td>\n      <td>4.0</td>\n      <td>0.74</td>\n      <td>...</td>\n      <td>0.27</td>\n      <td>0.09</td>\n      <td>4.21</td>\n      <td>2.43</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>13.38</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.942972</td>\n      <td>0.474534</td>\n      <td>0.784161</td>\n      <td>6.0</td>\n      <td>5.0</td>\n      <td>28.0</td>\n      <td>1.0</td>\n      <td>1.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.51</td>\n      <td>0.35</td>\n      <td>5.47</td>\n      <td>0.82</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>1.36</td>\n      <td>0.02</td>\n      <td>13.55</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.994065</td>\n      <td>0.459044</td>\n      <td>0.807674</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>69.0</td>\n      <td>2.0</td>\n      <td>1.30</td>\n      <td>1.0</td>\n      <td>0.02</td>\n      <td>...</td>\n      <td>0.37</td>\n      <td>0.28</td>\n      <td>2.20</td>\n      <td>1.48</td>\n      <td>0.19</td>\n      <td>0.00</td>\n      <td>1.74</td>\n      <td>0.00</td>\n      <td>14.32</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>0.997494</td>\n      <td>0.811975</td>\n      <td>0.910553</td>\n      <td>327.0</td>\n      <td>17.0</td>\n      <td>34.0</td>\n      <td>2.0</td>\n      <td>1.49</td>\n      <td>3.0</td>\n      <td>2.53</td>\n      <td>...</td>\n      <td>0.34</td>\n      <td>0.04</td>\n      <td>2.94</td>\n      <td>1.85</td>\n      <td>0.12</td>\n      <td>0.00</td>\n      <td>1.51</td>\n      <td>0.04</td>\n      <td>13.19</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>296</td>\n      <td>0.930048</td>\n      <td>0.592229</td>\n      <td>0.789775</td>\n      <td>72.0</td>\n      <td>0.0</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>1.00</td>\n      <td>2.0</td>\n      <td>0.06</td>\n      <td>...</td>\n      <td>0.33</td>\n      <td>0.37</td>\n      <td>2.03</td>\n      <td>1.27</td>\n      <td>0.11</td>\n      <td>0.03</td>\n      <td>1.23</td>\n      <td>0.00</td>\n      <td>12.65</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <td>297</td>\n      <td>0.995494</td>\n      <td>0.253093</td>\n      <td>0.821176</td>\n      <td>0.0</td>\n      <td>43.0</td>\n      <td>29.0</td>\n      <td>1.0</td>\n      <td>0.33</td>\n      <td>2.0</td>\n      <td>0.06</td>\n      <td>...</td>\n      <td>0.54</td>\n      <td>1.31</td>\n      <td>1.55</td>\n      <td>2.53</td>\n      <td>0.20</td>\n      <td>0.07</td>\n      <td>2.36</td>\n      <td>0.06</td>\n      <td>19.26</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <td>298</td>\n      <td>0.981666</td>\n      <td>0.806598</td>\n      <td>0.932583</td>\n      <td>447.0</td>\n      <td>9.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>1.87</td>\n      <td>3.0</td>\n      <td>2.56</td>\n      <td>...</td>\n      <td>0.18</td>\n      <td>0.07</td>\n      <td>4.11</td>\n      <td>0.76</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>1.15</td>\n      <td>0.00</td>\n      <td>9.01</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>299</td>\n      <td>0.986748</td>\n      <td>0.588679</td>\n      <td>0.833404</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>9.0</td>\n      <td>2.0</td>\n      <td>1.60</td>\n      <td>1.0</td>\n      <td>0.01</td>\n      <td>...</td>\n      <td>0.20</td>\n      <td>0.42</td>\n      <td>1.17</td>\n      <td>0.53</td>\n      <td>0.07</td>\n      <td>0.00</td>\n      <td>1.06</td>\n      <td>0.00</td>\n      <td>10.00</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>300 rows Ã— 47 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "preprocessing.convert_to_df(authors, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda4b7668ab7e68468d80b21b4a2524d1d7",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}