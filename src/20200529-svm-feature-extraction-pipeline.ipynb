{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "[E050] Can't find model '<spacy.lang.en.English object at 0x7fb4c9330a10>'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1ba967d9e3c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_md\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model '<spacy.lang.en.English object at 0x7fb4c9330a10>'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import data\n",
    "import random\n",
    "import features\n",
    "import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import go_to_project_root\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(spacy.load(\"en_core_web_md\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_time_augmentation(ids):\n",
    "    all_authors = data.get_raw_data()\n",
    "    _authors = [all_authors[_id] for _id in ids]\n",
    "    authors = {}\n",
    "\n",
    "    for a in _authors:\n",
    "        authors[a.author_id] = a\n",
    "\n",
    "    ones = [author for author in authors.values() if author.truth == 1]\n",
    "    zeros = [author for author in authors.values() if author.truth == 0]\n",
    "\n",
    "    tweets_1 = []\n",
    "    tweets_0 = []\n",
    "\n",
    "    for z in zeros:\n",
    "        for tweet in z.tweets:\n",
    "            tweets_0 += [tweet]\n",
    "    random.shuffle(tweets_0)\n",
    "\n",
    "    for o in ones:\n",
    "        for tweet in o.tweets:\n",
    "            tweets_1 += [tweet]\n",
    "    random.shuffle(tweets_1)\n",
    "\n",
    "    for author in zeros:\n",
    "        authors[author.author_id].tweets = []\n",
    "        for i in range(100):\n",
    "            authors[author.author_id].tweets += [tweets_0.pop(0)]\n",
    "\n",
    "    for author in ones:\n",
    "        authors[author.author_id].tweets = []\n",
    "        for i in range(100):\n",
    "            authors[author.author_id].tweets += [tweets_1.pop(0)]\n",
    "\n",
    "    for i, author in enumerate(authors.keys()):\n",
    "        authors[author].author_id = f\"shuffled-{i + 1}\"\n",
    "        \n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_augmentation(TestAuthor, n=3):\n",
    "    # Get in an author object\n",
    "    # Return a list of author objects created from a subset of tweets\n",
    "    Sub_Authors = [0] * n\n",
    "    for i in range(n):\n",
    "        # Randomly shuffle the tweets of the author\n",
    "        randomshuffle = random.shuffle(TestAuthor.tweets)\n",
    "        \n",
    "        #Save a new author object with half of the tweets\n",
    "        Sub_Authors[i] = Author(TestAuthor.author_id, randomshuffle[:50], TestAuthor.truth)\n",
    "        \n",
    "    return Sub_Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f9250b60d552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_lg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Beginning k fold 1\nAugmenting training data.\nLoading spacy data...\nExtracting semantic similarity. This may take some time...\n||||||||||"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-37b3e4f0aa36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Extract semantic similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0maugmentations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_semantic_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmentations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Get the lemmas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/NLP2/src/features.py\u001b[0m in \u001b[0;36mextract_semantic_similarity\u001b[0;34m(authors, model)\u001b[0m\n\u001b[1;32m     63\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                         \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                         \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.similarity\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/thinc/neural/util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcupy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mget_array_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# First we take a .csv file with the author IDs and their truth values\n",
    "\n",
    "go_to_project_root()\n",
    "\n",
    "all_authors = data.get_processed_data()\n",
    "\n",
    "df = pd.read_csv(\"data/IDs_names.csv\").to_numpy()\n",
    "X = df[:,0]\n",
    "y = df[:,1].astype(int)\n",
    "\n",
    "PIPELINE_PATH = \"data/processed/\"\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3,shuffle=True,random_state=69)\n",
    "\n",
    "#Start counting so we know in which fold we are in\n",
    "k = 1\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    print(\"Beginning k fold {}\".format(k))\n",
    "    \n",
    "    ############################ TRAINING ##############################\n",
    "    \n",
    "    Train_Authors = [all_authors[_id] for _id in X[train_index]]\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    print(\"Augmenting training data.\")\n",
    "    \n",
    "    # Augment training data. Then extract the features for it\n",
    "    augmentations = train_time_augmentation(X[train_index])\n",
    "    \n",
    "    # First extract the nonlinguistic features\n",
    "    augmentations = features.extract_nonlinguistic_features(augmentations)\n",
    "\n",
    "    # Extract semantic similarity\n",
    "    augmentations = features.extract_semantic_similarity(augmentations, model=nlp)\n",
    "\n",
    "    # Get the lemmas\n",
    "    augmentations = features.extract_clean_tweets(augmentations)\n",
    "\n",
    "    # Lexical features -- TTR requires lemmas\n",
    "    augmentations = features.extract_lexical_features(augmentations, model=nlp)\n",
    "\n",
    "    # Get Named Entities\n",
    "    augmentations = features.extract_named_entities(augmentations, model=nlp)\n",
    "\n",
    "    # Get POS tags\n",
    "    augmentations = features.extract_pos_tags(augmentations, model=nlp)\n",
    "\n",
    "    # Count POSes and get adjectives\n",
    "    augmentations = features.extract_POS_features(augmentations, model=nlp)\n",
    "\n",
    "    # Extract emotions\n",
    "    augmentations = features.extract_emotion_features(augmentations)\n",
    "    \n",
    "    ################# FEATURES ALL EXTRACTED ############\n",
    "    \n",
    "    print(\"Features have been extracted. Now clustering.\")\n",
    "    \n",
    "    Train_Authors.update(augmentations)\n",
    "    \n",
    "    # Cluster the Named Entities\n",
    "    Train_Authors, ner_clusters = features.extract_mcts_ner(Train_Authors)\n",
    "    \n",
    "    # Cluster the adjectives\n",
    "    Train_Authors, adj_clusters = features.extract_mcts_adj(Train_Authors)\n",
    "\n",
    "    # Create dataframe of what\n",
    "    train_df = preprocessing.convert_to_df(Train_Authors)\n",
    "    \n",
    "    \n",
    "    train_df = train_df.drop('author_id', axis=1).to_numpy()\n",
    "    X_train = train_df[:,:-1]\n",
    "    \n",
    "    THIS_PIPELINE_PATH = PIPELINE_PATH + \"K{}/\".format(k)\n",
    "    \n",
    "    print(\"Saving training data.\")\n",
    "    \n",
    "    X_train.to_csv(THIS_PIPELINE_PATH+\"X_train.csv\")\n",
    "    y_train.to_csv(THIS_PIPELINE_PATH+\"y_train.csv\")\n",
    "    \n",
    "    # Write clusters (if you want to further modularize this. you can save the test and train indices and separate these two parts of the feature extraction\n",
    "    with open('ner_clusters.txt', 'w') as f:\n",
    "        for item in ner_clusters:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "        \n",
    "    with open('adj_clusters.txt', 'w') as f:\n",
    "        for item in adj_clusters:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "        \n",
    "    ############################ TESTING DATA ##############################\n",
    "    \n",
    "    y_test = y[test_index]\n",
    "    preds = []\n",
    "    \n",
    "    # Save y_test values\n",
    "    y_test.to_csv(THIS_PIPELINE_PATH+\"y_test.csv\")\n",
    "\n",
    "    print(\"Beginning test time augmentation\")\n",
    "    \n",
    "    # First get all of the test authors\n",
    "    Test_Authors = data.get_raw_data(X[test_index])\n",
    "    \n",
    "    # Now to augment the test data\n",
    "    \n",
    "    # Go through each test data point once at a time\n",
    "    print(\"Extracting test author data.\")\n",
    "    for author in test_index:\n",
    "        Test3s_Authors = test_time_augmentation(Test_Authors[author])\n",
    "    \n",
    "        # We now have turned one test author into a dictionary of three authors.\n",
    "\n",
    "        # First extract the nonlinguistic features\n",
    "        Test3s_Authors = features.extract_nonlinguistic_features(Test3s_Authors)\n",
    "\n",
    "        # Extract semantic similarity\n",
    "        Test3s_Authors = features.extract_semantic_similarity(Test3s_Authors)\n",
    "\n",
    "        # Get the lemmas\n",
    "        Test3s_Authors = features.extract_clean_tweets(Test3s_Authors)\n",
    "\n",
    "        # Lexical features -- TTR requires lemmas\n",
    "        Test3s_Authors = features.extract_lexical_features(Test3s_Authors)\n",
    "\n",
    "        # Get Named Entities\n",
    "        Test3s_Authors = features.extract_named_entities(Test3s_Authors)\n",
    "\n",
    "        # Cluster the Named Entities\n",
    "        Test3s_Authors = features.extract_mcts_ner(Test3s_Authors, c=adj_clusters) ## NEW FUNCTION HERE\n",
    "\n",
    "        # Get POS tags\n",
    "        Test3s_Authors = features.extract_pos_tags(Test3s_Authors)\n",
    "\n",
    "        # Count POSes and get adjectives\n",
    "        Test3s_Authors = features.extract_POS_features(Test3s_Authors)\n",
    "\n",
    "        # Cluster the adjectives\n",
    "        Test3s_Authors = features.extract_mcts_adj(Test3s_Authors, c=ner_clusters) ## NEW FUNCTION HERE\n",
    "\n",
    "        # Extract emotions\n",
    "        Test3s_Authors = features.extract_emotion_features(Test3s_Authors)\n",
    "\n",
    "        test_df = preprocessing.convert_to_df(Test3s_Authors)\n",
    "        test_df = test_df.drop('author_id', axis=1).to_numpy()\n",
    "        X_test = test_df[:,:-1]\n",
    "        \n",
    "        THIS_PIPELINE_PATH = THIS_PIPELINE_PATH + \"X_test/\"\n",
    "        \n",
    "        # Save all three datapoints corresponding to this author in a CSV file in a folder called \"X_test\"\n",
    "        X_test.to_csv(THIS_PIPELINE_PATH+\"{author}.csv\")\n",
    "        print(\"|\",end=\"\")\n",
    "        \n",
    "    print()\n",
    "    print(\"Batch {k} finished\")\n",
    "    print()\n",
    "    print()\n",
    "    k += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda4b7668ab7e68468d80b21b4a2524d1d7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}