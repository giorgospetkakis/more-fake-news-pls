{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import random\n",
    "import features\n",
    "import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_time_augmentation(ids):\n",
    "    authors = data.get_raw_data(ids)\n",
    "    ones = [author for author in authors.values() if author.truth == 1]\n",
    "    zeros = [author for author in authors.values() if author.truth == 0]\n",
    "\n",
    "    tweets_1 = []\n",
    "    tweets_0 = []\n",
    "\n",
    "    for z in zeros:\n",
    "        for tweet in z.tweets:\n",
    "            tweets_0 += [tweet]\n",
    "    random.shuffle(tweets_0)\n",
    "\n",
    "    for o in ones:\n",
    "        for tweet in o.tweets:\n",
    "            tweets_1 += [tweet]\n",
    "    random.shuffle(tweets_1)\n",
    "\n",
    "    for author in zeros:\n",
    "        authors[author.author_id].tweets = []\n",
    "        for i in range(100):\n",
    "            authors[author.author_id].tweets += [tweets_0.pop(0)]\n",
    "\n",
    "    for author in ones:\n",
    "        authors[author.author_id].tweets = []\n",
    "        for i in range(100):\n",
    "            authors[author.author_id].tweets += [tweets_1.pop(0)]\n",
    "\n",
    "    for i, author in enumerate(authors.keys()):\n",
    "        authors[author].author_id = f\"shuffled-{i + 1}\"\n",
    "        \n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_augmentation(TestAuthor, n=3):\n",
    "    # Get in an author object\n",
    "    # Return a dict of author objects created from a subset of tweets\n",
    "    Sub_Authors = {}\n",
    "    for i in range(n):\n",
    "        # Randomly shuffle the tweets of the author\n",
    "        randomshuffle = random.shuffle(TestAuthor.tweets)\n",
    "        \n",
    "        #Save a new author object with half of the tweets\n",
    "        Sub_Author[i] = Author(TestAuthor.author_id, randomshuffle[:50], TestAuthor.truth)\n",
    "        \n",
    "    return Sub_Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we take a .csv file with the author IDs and their truth values\n",
    "df = pd.read_csv(\"IDs_names.csv\")\n",
    "X = df[:,0]\n",
    "y = df[:,1]\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3,shuffle=True,random_state=69)\n",
    "\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    \n",
    "    ############################ TRAINING ##############################\n",
    "    \n",
    "    Train_Authors = data.get_raw_data(X[train_index]) #will return list of author IDs, how can we use this to determine which author files we take up\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    # Augment training data and then append the augmentation to the dictionary of authors\n",
    "    augmentations = train_time_augmentation(X[train_index])\n",
    "    Train_Authors.update(augmentations)\n",
    "    \n",
    "    # First extract the nonlinguistic features\n",
    "    Train_Authors = features.extract_nonlinguistic_features(Train_Authors)\n",
    "\n",
    "    # Extract semantic similarity\n",
    "    Train_Authors = feature.extract_semantic_similarity(Train_Authors)\n",
    "\n",
    "    # Get the lemmas\n",
    "    Train_Authors = features.extract_clean_tweets(Train_Authors)\n",
    "\n",
    "    # Lexical features -- TTR requires lemmas\n",
    "    Train_Authors = features.extract_lexical_features(Train_Authors)\n",
    "\n",
    "    # Get Named Entities\n",
    "    Train_Authors = features.extract_named_entities(Train_Authors)\n",
    "\n",
    "    # Cluster the Named Entities\n",
    "    Train_Authors = features.extract_mcts_ner(Train_Authors)\n",
    "\n",
    "    # Get POS tags\n",
    "    Train_Authors = features.extract_pos_tags(Train_Authors)\n",
    "\n",
    "    # Count POSes and get adjectives\n",
    "    Train_Authors = features.extract_POS_features(Train_Authors)\n",
    "\n",
    "    # Cluster the adjectives\n",
    "    Train_Authors = features.extract_mcts_adj(Train_Authors)\n",
    "\n",
    "    # MORE FEATURES?\n",
    "\n",
    "    train_df = preprocessing.convert_to_df(Train_Authors)\n",
    "    \n",
    "    \n",
    "    train_df = train_df.drop('author_id', axis=1).to_numpy()\n",
    "    X_train = train_df[:,:-1]\n",
    "    \n",
    "    # TRAIN MODEL HERE. CHOOSE MODEL --> LOGISTIC REGRESSION W ADAM? XGBOOST?\n",
    "    \n",
    "    ############################ TESTING ##############################\n",
    "    y_test = y[test_index]\n",
    "    preds = []\n",
    "    \n",
    "    # First get all of the test authors\n",
    "    Test_Authors = data.get_raw_data(X[test_index])\n",
    "    \n",
    "    # Now to augment the test data\n",
    "    \n",
    "    # Go through each test data point once at a time\n",
    "    for author in test_index:\n",
    "        Test3s_Authors = test_time_augmentation(Test_Authors[author])\n",
    "    \n",
    "        # We now have three datapoints for every author datapoint.\n",
    "\n",
    "        # Extract test features\n",
    "\n",
    "        # First extract the nonlinguistic features\n",
    "        Test3s_Authors = features.extract_nonlinguistic_features(Test3s_Authors)\n",
    "\n",
    "        # Extract semantic similarity\n",
    "        Test3s_Authors = feature.extract_semantic_similarity(Test3s_Authors)\n",
    "\n",
    "        # Get the lemmas\n",
    "        Test3s_Authors = features.extract_clean_tweets(Test3s_Authors)\n",
    "\n",
    "        # Lexical features -- TTR requires lemmas\n",
    "        Test3s_Authors = features.extract_lexical_features(Test3s_Authors)\n",
    "\n",
    "        # Get Named Entities\n",
    "        Test3s_Authors = features.extract_named_entities(Test3s_Authors)\n",
    "\n",
    "        # Cluster the Named Entities\n",
    "        Test3s_Authors = features.extract_mcts_ner(Test3s_Authors) ## COMPARE TO TRAINED CLUSTERS FOR NER MAKE NEW FUNCTION\n",
    "\n",
    "        # Get POS tags\n",
    "        Test3s_Authors = features.extract_pos_tags(Test3s_Authors)\n",
    "\n",
    "        # Count POSes and get adjectives\n",
    "        Test3s_Authors = features.extract_POS_features(Test3s_Authors)\n",
    "\n",
    "        # Cluster the adjectives\n",
    "        Test3s_Authors = features.extract_mcts_adj(Test3s_Authors) ## COMPARE TO TRAINED CLUSTERS FOR NEW MAKE NEW FUNCTION\n",
    "\n",
    "        # MORE FEATURES?\n",
    "\n",
    "        test_df = preprocessing.convert_to_df(Test3s_Authors)\n",
    "        test_df = test_df.drop('author_id', axis=1).to_numpy()\n",
    "        X_test = test_df[:,:-1]\n",
    "        y_t3st = test_df[:,-1]\n",
    "\n",
    "        # TEST MODEL HERE NEED TO DETERMINE IT --> name classifications as pr3d\n",
    "\n",
    "        # Reduce our augmented test data into one and append that to our prediction list\n",
    "        preds.append(stats.mode(pr3d))\n",
    "    \n",
    "    # Print the accuracy of the entire validation set\n",
    "    print(accuracy_score(preds,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
